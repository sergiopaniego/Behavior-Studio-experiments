{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BS_classification_network_sequence_extreme_dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN/elw5Cg06eVf60UjEzdHK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiopaniego/BehaviorStudio-experiments/blob/main/BS_classification_network_sequence_extreme_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQU41fYJSAhy",
        "outputId": "ee5d2ddf-02e3-4335-c443-e248f7b7152e"
      },
      "source": [
        "# When using Colab, check the GPU that is assigned and reload the runtime if its memory is low\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Mar  4 18:20:27 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    76W / 149W |   1807MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ibWmo-WSMmH",
        "outputId": "cb89d5fd-d573-49f1-8616-c678c7bff04f"
      },
      "source": [
        "# Mount Google Drive to access images dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egUAtW3FSPk1",
        "outputId": "0c5401d2-61fd-49f1-a787-93fa3740f6a8"
      },
      "source": [
        "!ls \"/content/drive/My Drive\"\n",
        "!ls \"/content/drive/My Drive/complete_dataset.zip\"\n",
        "!unzip \"/content/drive/My Drive/curves_only.zip\"\n",
        "!unzip \"/content/drive/My Drive/complete_dataset.zip\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 20210215-134322_model_lstm_sequence_test_cp.h5\n",
            " 20210215-134322_model_lstm_sequence_test.csv\n",
            " 20210215-134625_model_lstm_sequence_test_cp.h5\n",
            " 20210215-134625_model_lstm_sequence_test.csv\n",
            " 20210215-134625_model_lstm_sequence_test.h5\n",
            " 20210215-180738_deepest_lstm_tinypilotnet_model_sequence_cp.h5\n",
            " 20210215-180738_deepest_lstm_tinypilotnet_model_sequence.csv\n",
            " 20210215-180738_deepest_lstm_tinypilotnet_model_sequence.h5\n",
            " 20210216-084753_tinypilotnet_lstm_model_sequence_cp.h5\n",
            " 20210216-084753_tinypilotnet_lstm_model_sequence.csv\n",
            " 20210216-084753_tinypilotnet_lstm_model_sequence.h5\n",
            " 20210216-091101_tinypilotnet_model_sequence_cp.h5\n",
            " 20210216-091101_tinypilotnet_model_sequence.csv\n",
            " 20210216-091101_tinypilotnet_model_sequence.h5\n",
            " 20210216-095459_pilotnet_model_model_sequence_cp.h5\n",
            " 20210216-095459_pilotnet_model_model_sequence.csv\n",
            " 20210216-095459_pilotnet_model_model_sequence.h5\n",
            " 20210219-082603_deepest_lstm_tinypilotnet_model_RGB_sequence_cp.h5\n",
            " 20210219-082603_deepest_lstm_tinypilotnet_model_RGB_sequence.csv\n",
            " 20210219-082603_deepest_lstm_tinypilotnet_model_RGB_sequence.h5\n",
            " 20210219-164202_pilotnet_model_model_RGB_sequence_cp.h5\n",
            " 20210219-164202_pilotnet_model_model_RGB_sequence.csv\n",
            " 20210219-164202_pilotnet_model_model_RGB_sequence.h5\n",
            " 20210219-164420_tinypilotnet_lstm_model_RGB_sequence.csv\n",
            " 20210219-164446_tinypilotnet_model_RGB_sequence_cp.h5\n",
            " 20210219-164446_tinypilotnet_model_RGB_sequence.csv\n",
            " 20210219-164446_tinypilotnet_model_RGB_sequence.h5\n",
            " 20210219-171505_tinypilotnet_lstm_model_RGB_sequence_cp.h5\n",
            " 20210219-171505_tinypilotnet_lstm_model_RGB_sequence.csv\n",
            " 20210219-171505_tinypilotnet_lstm_model_RGB_sequence.h5\n",
            " 20210222-145945_deepest_lstm_tinypilotnet_model_RGB_sequence.csv\n",
            " 20210222-150042_deepest_lstm_tinypilotnet_model_RGB_sequence_cp.h5\n",
            " 20210222-150042_deepest_lstm_tinypilotnet_model_RGB_sequence.csv\n",
            " 20210222-150042_deepest_lstm_tinypilotnet_model_RGB_sequence.h5\n",
            " 20210222-160448_deepest_lstm_tinypilotnet_model_300_RGB_sequence_cp.h5\n",
            " 20210222-160448_deepest_lstm_tinypilotnet_model_300_RGB_sequence.csv\n",
            " 20210222-161338_tinypilotnet_lstm_model_300_RGB_sequence_cp.h5\n",
            " 20210222-161338_tinypilotnet_lstm_model_300_RGB_sequence.csv\n",
            " 20210222-161338_tinypilotnet_lstm_model_300_RGB_sequence.h5\n",
            " 20210222-173243_tinypilotnet_model_300_RGB_sequence_cp.h5\n",
            " 20210222-173243_tinypilotnet_model_300_RGB_sequence.csv\n",
            " 20210222-173243_tinypilotnet_model_300_RGB_sequence.h5\n",
            " 20210222-175955_pilotnet_model_model_300_RGB_sequence_cp.h5\n",
            " 20210222-175955_pilotnet_model_model_300_RGB_sequence.csv\n",
            " 20210222-175955_pilotnet_model_model_300_RGB_sequence.h5\n",
            " 20210222-183328_deepest_lstm_tinypilotnet_model_300_RGB_sequence_cp.h5\n",
            " 20210222-183328_deepest_lstm_tinypilotnet_model_300_RGB_sequence.csv\n",
            " 20210222-183328_deepest_lstm_tinypilotnet_model_300_RGB_sequence.h5\n",
            " 20210224-094211_tinypilotnet_model_25_RGB_sequence_cp.h5\n",
            " 20210224-094211_tinypilotnet_model_25_RGB_sequence.csv\n",
            " 20210224-094211_tinypilotnet_model_25_RGB_sequence.h5\n",
            " 20210224-100616_tinypilotnet_model_50_RGB_sequence.csv\n",
            " 20210224-101345_tinypilotnet_model_50_RGB_sequence_cp.h5\n",
            " 20210224-101345_tinypilotnet_model_50_RGB_sequence.csv\n",
            " 20210224-101345_tinypilotnet_model_50_RGB_sequence.h5\n",
            " 20210224-101937_tinypilotnet_model_50_RGB_sequence_cp.h5\n",
            " 20210224-101937_tinypilotnet_model_50_RGB_sequence.csv\n",
            " 20210224-101937_tinypilotnet_model_50_RGB_sequence.h5\n",
            " 20210224-102723_tinypilotnet_model_300_RGB_sequence_cp.h5\n",
            " 20210224-102723_tinypilotnet_model_300_RGB_sequence.csv\n",
            " 20210224-102723_tinypilotnet_model_300_RGB_sequence.h5\n",
            " 20210224-110142_tinypilotnet_model_300_RGB_sequence_cp.h5\n",
            " 20210224-110142_tinypilotnet_model_300_RGB_sequence.csv\n",
            " 20210224-110142_tinypilotnet_model_300_RGB_sequence.h5\n",
            " 20210226-100325_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch_cp.h5\n",
            " 20210226-100325_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch.csv\n",
            " 20210226-100325_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch.h5\n",
            " 20210226-103100_tinypilotnet_model_300_RGB_sequence_extreme_cp.h5\n",
            " 20210226-103100_tinypilotnet_model_300_RGB_sequence_extreme.csv\n",
            " 20210226-103100_tinypilotnet_model_300_RGB_sequence_extreme.h5\n",
            " 20210301-153401_pilotnet_model_model_300_RGB_extreme_sequence_cp.h5\n",
            " 20210301-153401_pilotnet_model_model_300_RGB_extreme_sequence.csv\n",
            " 20210301-153401_pilotnet_model_model_300_RGB_extreme_sequence.h5\n",
            " 20210301-172225_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch_cp.h5\n",
            " 20210301-172225_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch.csv\n",
            " 20210301-172225_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch.h5\n",
            " 20210301-174942_tinypilotnet_lstm_model_300_RGB_sequence_cp.h5\n",
            " 20210301-174942_tinypilotnet_lstm_model_300_RGB_sequence.csv\n",
            " 20210301-174942_tinypilotnet_lstm_model_300_RGB_sequence.h5\n",
            " 20210302-082158_tinypilotnet_lstm_model_300_RGB_sequence_cp.h5\n",
            " 20210302-082158_tinypilotnet_lstm_model_300_RGB_sequence.csv\n",
            " 20210302-082158_tinypilotnet_lstm_model_300_RGB_sequence.h5\n",
            " 20210302-082208_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch_cp.h5\n",
            " 20210302-082208_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch.csv\n",
            " 20210302-082208_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch.h5\n",
            " 20210302-121834_deepest_lstm_tinypilotnet_model_50_RGB_sequence_cp.h5\n",
            " 20210302-121834_deepest_lstm_tinypilotnet_model_50_RGB_sequence.csv\n",
            " 20210302-121834_deepest_lstm_tinypilotnet_model_50_RGB_sequence.h5\n",
            " 20210302-123811_deepest_lstm_tinypilotnet_model_300_RGB_sequence_cp.h5\n",
            " 20210302-123811_deepest_lstm_tinypilotnet_model_300_RGB_sequence.csv\n",
            " 20210302-123811_deepest_lstm_tinypilotnet_model_300_RGB_sequence.h5\n",
            " 20210304-165320_smaller_vgg_net_model_300_RGB_extreme_sequence.csv\n",
            " 20210304-165438_smaller_vgg_net_model_300_RGB_extreme_sequence.csv\n",
            " 20210304-165744_smaller_vgg_net_model_300_RGB_extreme_sequence.csv\n",
            " 20210304-165850_smaller_vgg_net_model_300_RGB_extreme_sequence.csv\n",
            " 20210304-170448_smaller_vgg_net_model_300_RGB_extreme_sequence_cp.h5\n",
            " 20210304-170448_smaller_vgg_net_model_300_RGB_extreme_sequence.csv\n",
            " 20210304-170851_smaller_vgg_net_model_300_RGB_extreme_sequence_cp.h5\n",
            " 20210304-170851_smaller_vgg_net_model_300_RGB_extreme_sequence.csv\n",
            " AI-exam.zip\n",
            " certificados-horas-formativas-doctorado\n",
            "'Colab Notebooks'\n",
            " complete_dataset.zip\n",
            " curves_only.zip\n",
            " DetectionStudio\n",
            "'documentación doctorado'\n",
            " Draw.io\n",
            "'Fotos Film'\n",
            " many_curves_dataset.zip\n",
            " merged_model_pilotnet_cropped_100_dense_1.h5\n",
            " merged_model_pilotnet_cropped_100_dense_2.h5\n",
            " merged_model_pilotnet_cropped_100.h5\n",
            " merged_model_tinypilotnet_cropped_100.h5\n",
            " merged_model_tinypilotnet_cropped_300.h5\n",
            " model_deepest_lstm_cropped_250_norm_max_pooling.h5\n",
            " model_deepest_lstm_cropped_250_norm_test.h5\n",
            " model_lstm_cropped_1.h5\n",
            " model_lstm_cropped_1_test.h5\n",
            " model_lstm_cropped_250_checkpoint_norm_test_lstm.h5\n",
            " model_lstm_cropped_250_norm_test_lstm.h5\n",
            " model_lstm_cropped_300_norm.h5\n",
            " model_lstm_cropped_500_norm_test_lstm.h5\n",
            " model_lstm_sequence_dataset.h5\n",
            " model_lstm_sequence_test_cp.h5\n",
            " model_lstm_sequence_test.csv\n",
            " model_lstm_sequence_test.h5\n",
            " model_lstm_tinypilotnet_checkpoint_1000.h5\n",
            " model_lstm_tinypilotnet_checkpoint_100.h5\n",
            " model_lstm_tinypilotnet_checkpoint_1200.h5\n",
            " model_lstm_tinypilotnet_checkpoint_1700.h5\n",
            " model_lstm_tinypilotnet_checkpoint_500.h5\n",
            " model_lstm_tinypilotnet_cropped_150.h5\n",
            " model_lstm_tinypilotnet_cropped_150_v.h5\n",
            " model_lstm_tinypilotnet_cropped_150_w.h5\n",
            " model_lstm_tinypilotnet_cropped_25.h5\n",
            " model_lstm_tinypilotnet_cropped_50.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_1000.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_100.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_10_2.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_150.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_200.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_300_2.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_50_2.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_norm_300_red.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_norm_500.h5\n",
            " model_pilotnet_cropped_300.h5\n",
            " model_pilotnet_cropped_300_v.h5\n",
            " model_pilotnet_cropped_300_w.h5\n",
            " model_pilotnet_cropped_v.h5\n",
            " model_pilotnet_cropped_w.h5\n",
            " model_plot.png\n",
            " model_smaller_vgg_7classes_normal_w.h5\n",
            " model_smaller_vgg.png\n",
            " model_tinypilotnet_cropped_v.h5\n",
            " model_tinypilotnet_cropped_w.h5\n",
            " Personal\n",
            " recovery-keys\n",
            " simple_circuit_dataset.zip\n",
            " test_model_tf_keras_balanced_cropped_v.h5\n",
            " test_model_tf_keras_balanced_croppedv.h5\n",
            " test_model_tf_keras_balanced_croppedw.h5\n",
            " test_model_tf_keras_balanced_v.h5\n",
            " test_model_tf_keras_balanced_w.h5\n",
            " test_model_tf_keras_cropped_biased_v.h5\n",
            " test_model_tf_keras_cropped_biased_w.h5\n",
            " test_model_tf_keras.h5\n",
            " test_model_tf_keras_v.h5\n",
            " test_model_tf_keras_w.h5\n",
            " URJC\n",
            " websim-2-robots.mov\n",
            "'/content/drive/My Drive/complete_dataset.zip'\n",
            "Archive:  /content/drive/My Drive/curves_only.zip\n",
            "replace curves_only/data.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  /content/drive/My Drive/complete_dataset.zip\n",
            "replace complete_dataset/Train_balanced_bbdd_v/train.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5OgFc1vnBUT",
        "outputId": "c531615d-a9b3-4960-a572-bb336eba01e3"
      },
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import plot_model, np_utils\n",
        "\n",
        "def load_data(folder):\n",
        "    name_folder = '/content/' + folder + '/Images/'\n",
        "    list_images = glob.glob(name_folder + '*')\n",
        "    #print(list_images)\n",
        "    images = sorted(list_images, key=lambda x: int(x.split('/')[4].split('.png')[0]))\n",
        "    name_file = '/content/' + folder + '/data.json'\n",
        "    file = open(name_file, 'r')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return images, data\n",
        "\n",
        "\n",
        "def get_images(list_images, type_image, array_imgs):\n",
        "    # Read the images\n",
        "    for name in list_images:\n",
        "        img = cv2.imread(name)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        if type_image == 'cropped':\n",
        "            img = img[240:480, 0:640]\n",
        "        img = cv2.resize(img, (int(img.shape[1] / 4), int(img.shape[0] / 4)))\n",
        "        array_imgs.append(img)\n",
        "\n",
        "    return array_imgs\n",
        "\n",
        "\n",
        "def parse_json_7_classes_w(data):\n",
        "    # Process json 7 classes for w\n",
        "    array_class = []\n",
        "    data_parse = data.split('\"class2\": \"')[1:]\n",
        "    for d in data_parse:\n",
        "        classification = d.split('\", \"')[0]\n",
        "        array_class.append(classification)\n",
        "    \n",
        "    return array_class\n",
        "\n",
        "def parse_json_5_classes_v(data):\n",
        "    # Process json 5 classes for v\n",
        "    array_class = []\n",
        "    data_parse = data.split('\"class3\": \"')[1:]\n",
        "    for d in data_parse:\n",
        "        classification = d.split('\", \"')[0]\n",
        "        array_class.append(classification)\n",
        "    return array_class\n",
        "\n",
        "def parse_json(data, array):\n",
        "    array_class_w = parse_json_7_classes_w(data)\n",
        "    array_class_v = parse_json_5_classes_v(data)\n",
        "\n",
        "    #array_class_v_w = []\n",
        "    #for i in range(0, len(array_class_w)):\n",
        "      #array_class_v_w.append([array_class_v[i], array_class_w[i]])\n",
        "\n",
        "    for i in range(0, len(array_class_w)):\n",
        "      array.append([array_class_v[i], array_class_w[i]])\n",
        "\n",
        "    # array_class = [array_class_v, array_class_w]\n",
        "    # array_class = adapt_labels(array_class_v_w, array)\n",
        "\n",
        "    return array\n",
        "\n",
        "\n",
        "def adapt_labels(array_labels, array_classes):\n",
        "  for i in range(0, len(array_labels)):\n",
        "    label_v = array_labels[i][0]\n",
        "    label_w = array_labels[i][1]\n",
        "    if label_v == 'slow' and label_w == 'radically_left':\n",
        "      array_classes.append(0)\n",
        "    elif label_v == 'slow' and label_w == 'moderately_left':\n",
        "      array_classes.append(1)\n",
        "    elif label_v == 'slow' and label_w == 'slightly_left':\n",
        "      array_classes.append(2)\n",
        "    elif label_v == 'slow' and label_w == 'slight':\n",
        "      array_classes.append(3)\n",
        "    elif label_v == 'slow' and label_w == 'slightly_right':\n",
        "      array_classes.append(4)\n",
        "    elif label_v == 'slow' and label_w == 'moderately_right':\n",
        "      array_classes.append(5)\n",
        "    elif label_v == 'slow' and label_w == 'radically_right':\n",
        "      array_classes.append(6)\n",
        "    elif label_v == 'moderate' and label_w == 'radically_left':\n",
        "      array_classes.append(7)\n",
        "    elif label_v == 'moderate' and label_w == 'moderately_left':\n",
        "      array_classes.append(8)\n",
        "    elif label_v == 'moderate' and label_w == 'slightly_left':\n",
        "      array_classes.append(9)\n",
        "    elif label_v == 'moderate' and label_w == 'slight':\n",
        "      array_classes.append(10)\n",
        "    elif label_v == 'moderate' and label_w == 'slightly_right':\n",
        "      array_classes.append(11)\n",
        "    elif label_v == 'moderate' and label_w == 'moderately_right':\n",
        "      array_classes.append(12)\n",
        "    elif label_v == 'moderate' and label_w == 'radically_right':\n",
        "      array_classes.append(13)\n",
        "    elif label_v == 'fast' and label_w == 'radically_left':\n",
        "      array_classes.append(14)\n",
        "    elif label_v == 'fast' and label_w == 'moderately_left':\n",
        "      array_classes.append(15)\n",
        "    elif label_v == 'fast' and label_w == 'slightly_left':\n",
        "      array_classes.append(16)\n",
        "    elif label_v == 'fast' and label_w == 'slight':\n",
        "      array_classes.append(17)\n",
        "    elif label_v == 'fast' and label_w == 'slightly_right':\n",
        "      array_classes.append(18)\n",
        "    elif label_v == 'fast' and label_w == 'moderately_right':\n",
        "      array_classes.append(19)\n",
        "    elif label_v == 'fast' and label_w == 'radically_right':\n",
        "      array_classes.append(20)\n",
        "    elif label_v == 'very_fast' and label_w == 'radically_left':\n",
        "      array_classes.append(21)\n",
        "    elif label_v == 'very_fast' and label_w == 'moderately_left':\n",
        "      array_classes.append(22)\n",
        "    elif label_v == 'very_fast' and label_w == 'slightly_left':\n",
        "      array_classes.append(23)\n",
        "    elif label_v == 'very_fast' and label_w == 'slight':\n",
        "      array_classes.append(24)\n",
        "    elif label_v == 'very_fast' and label_w == 'slightly_right':\n",
        "      array_classes.append(25)\n",
        "    elif label_v == 'very_fast' and label_w == 'moderately_right':\n",
        "      array_classes.append(26)\n",
        "    elif label_v == 'very_fast' and label_w == 'radically_right':\n",
        "      array_classes.append(27)\n",
        "  \n",
        "  return array_classes\n",
        "\n",
        "\n",
        "def preprocess_data(array, imgs):\n",
        "    # Data augmentation\n",
        "    # Take the image and just flip it and negate the measurement\n",
        "    flip_imgs = []\n",
        "    array_flip = []\n",
        "    for i in range(len(array)):\n",
        "        flip_imgs.append(cv2.flip(imgs[i], 1))\n",
        "        if array[i][1] == 'radically_left':\n",
        "          w = 'radically_right'\n",
        "        elif array[i][1] == 'moderately_left':\n",
        "          w = 'moderately_right'\n",
        "        elif array[i][1] == 'slightly_left':\n",
        "          w = 'slightly_right'\n",
        "        elif array[i][1] == 'slight':\n",
        "          w = 'slight'\n",
        "        elif array[i][1] == 'slightly_right':\n",
        "          w = 'slightly_left'\n",
        "        elif array[i][1] == 'moderately_right':\n",
        "          w = 'moderately_left'\n",
        "        elif array[i][1] == 'radically_right':\n",
        "          w = 'radically_left'\n",
        "        array_flip.append((array[i][0], w))\n",
        "    new_array = array + array_flip\n",
        "    new_array_imgs = imgs + flip_imgs\n",
        "    return new_array, new_array_imgs\n",
        "\n",
        "def add_extreme_data(array, imgs):\n",
        "    for i in range(0, len(array)):\n",
        "      if array[i][1] == 'radically_right' or array[i][1] == 'radically_left':\n",
        "        array.append(array[i])\n",
        "        imgs.append(imgs[i])\n",
        "    return array, imgs\n",
        "\n",
        "\n",
        "# Load data\n",
        "images, data = load_data('complete_dataset')\n",
        "images_curve, data_curve = load_data('curves_only')\n",
        "\n",
        "# CHANGE type_image\n",
        "type_image = 'cropped'\n",
        "#type_image='normal'\n",
        "\n",
        "# Preprocess images\n",
        "array_imgs = []\n",
        "array_imgs = get_images(images, type_image, array_imgs)\n",
        "array_imgs = get_images(images_curve, type_image, array_imgs)\n",
        "# Preprocess json\n",
        "array_annotations = []\n",
        "array_annotations = parse_json(data, array_annotations)\n",
        "print(len(array_annotations))\n",
        "array_annotations = parse_json(data_curve, array_annotations)\n",
        "print(len(array_annotations))\n",
        "\n",
        "\n",
        "if type_image == 'cropped':\n",
        "    img_shape = (65, 160, 3)\n",
        "else:\n",
        "    img_shape = (120, 160, 3)\n",
        "\n",
        "\n",
        "# Adapt the data\n",
        "array_annotations, array_imgs = preprocess_data(array_annotations, array_imgs)\n",
        "print(len(array_annotations))\n",
        "array_annotations, array_imgs = add_extreme_data(array_annotations, array_imgs)\n",
        "print(len(array_annotations))\n",
        "\n",
        "\n",
        "array_classes = []\n",
        "array_classes = adapt_labels(array_annotations, array_classes)\n",
        "print(len(array_classes))\n",
        "print(array_classes[0])\n",
        "\n",
        "split_test_train_value = 0.30\n",
        "images_train, images_validation, classes_train, classes_validation = train_test_split(array_imgs, array_classes, test_size=split_test_train_value, random_state=42, shuffle=True)\n",
        "# Adapt the data\n",
        "images_train = np.stack(images_train, axis=0)\n",
        "images_validation = np.stack(images_validation, axis=0)\n",
        "\n",
        "classes_train = np_utils.to_categorical(classes_train, 28)\n",
        "classes_validation = np_utils.to_categorical(classes_validation, 28)\n",
        "\n",
        "classes_train = np.stack(classes_train, axis=0)\n",
        "classes_validation = np.stack(classes_validation, axis=0)\n",
        "\n",
        "\n",
        "print(classes_train[0])\n",
        "print(classes_train.shape)\n",
        "\n",
        "\n",
        "print(images_train.shape)\n",
        "print(images_validation.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17341\n",
            "22609\n",
            "45218\n",
            "49378\n",
            "49378\n",
            "24\n",
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "(34564, 28)\n",
            "(34564, 60, 160, 3)\n",
            "(14814, 60, 160, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6At55Lt_SP7n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "outputId": "339ce119-9e79-43c1-bdb7-f984881fae6c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(classes_train[100], classes_validation[100])\n",
        "\n",
        "\n",
        "plt.show()\n",
        "n, bins, patches = plt.hist(x=array_classes, bins='auto')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#n, bins, patches = plt.hist(x=train_ann_x, bins='auto')\n",
        "plt.show()\n",
        "n, bins, patches = plt.hist(x=classes_train, bins='auto')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "plt.show()\n",
        "n, bins, patches = plt.hist(x=classes_validation, bins='auto')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATVUlEQVR4nO3df6zd9X3f8eeruCRNm2AT31Jme7XXuEkcsin0ClxF6rq4MoZGGGk0NVqLk3mx1pCua6Ol0EqzBUVq1KWsaAmZG9yYKOPHaDushdSzgIhtigmXkBB+lHILCb4exLexcaahJHX63h/n4/bEvdf33nPuPcf2fT6ko/v9vr+f7/m+P9jwut8f55CqQpK0uP3AsBuQJA2fYSBJMgwkSYaBJAnDQJIELBl2A71avnx5rV69ethtSNIZ5bHHHvurqho5uX7GhsHq1asZGxsbdhuSdEZJ8vWp6l4mkiQZBpIkw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShgEPPPgTALx9z9v52L9+EIAfe+jLsPM8ACau/59D602SBmXRh4EkyTCQJLGIw+CZt7x12C1I0mlj0YaBJOnvGAaSJMNAkmQYSJKYRRgk2Z3kcJInp9j2oSSVZHlbT5Jbk4wneSLJxV1jtyZ5rr22dtV/KslX2z63Jsl8TU6SNDuzOTP4FLDp5GKSVcBG4MWu8uXA2vbaDtzWxp4P7AAuBS4BdiRZ1va5DXh/135/71iSpIU1YxhU1cPAkSk23QJ8GKiu2mbgjuo4ACxNciFwGbC/qo5U1VFgP7CpbXtDVR2oqgLuAK7qb0pzt/r6z05ZP/GJZEk62/V0zyDJZuBQVX3lpE0rgINd6xOtdqr6xBT16Y67PclYkrHJycleWpckTWHOYZDkdcBvAf9+/ts5taraVVWjVTU6MjIy6MNL0lmrlzODnwDWAF9J8jVgJfClJD8GHAJWdY1d2Wqnqq+coi5JGqA5h0FVfbWqfrSqVlfVajqXdi6uqpeBvcC17ami9cCxqnoJ2AdsTLKs3TjeCOxr276VZH17iuha4L55mltPdu7cOczDS9JQzObR0juBLwBvTjKRZNspht8PPA+MA38IfACgqo4ANwGPtteNrUYb88m2z18Cn+ttKpKkXi2ZaUBVXTPD9tVdywVcN8243cDuKepjwEUz9SFJWjh+AlmSZBhIkgwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAksRiDYOd5wHw0V9895AbkaTTw+IMA0nS9zEMJEmGgSTJMJAkYRhIkphFGCTZneRwkie7ar+X5M+TPJHkT5Ms7dp2Q5LxJM8muayrvqnVxpNc31Vfk+SRVr87ybnzOUFJ0sxmc2bwKWDTSbX9wEVV9Y+BvwBuAEiyDtgCvK3t8/Ek5yQ5B/gYcDmwDrimjQX4CHBLVb0JOAps62tGkqQ5mzEMquph4MhJtf9RVcfb6gFgZVveDNxVVd+pqheAceCS9hqvquer6rvAXcDmJAHeBdzb9t8DXNXnnCRJczQf9wz+JfC5trwCONi1baLVpqu/EXilK1hO1KeUZHuSsSRjk5OT89C6JAn6DIMkvw0cBz4zP+2cWlXtqqrRqhodGRkZxCElaVFY0uuOSd4LvBvYUFXVyoeAVV3DVrYa09S/CSxNsqSdHXSPlyQNSE9nBkk2AR8GrqyqV7s27QW2JHlNkjXAWuCLwKPA2vbk0Ll0bjLvbSHyEHB1238rcF9vU5Ek9Wo2j5beCXwBeHOSiSTbgP8EvB7Yn+TLST4BUFVPAfcATwN/BlxXVd9rv/V/ENgHPAPc08YC/CbwG0nG6dxDuH1eZyhJmtGMl4mq6popytP+B7uqbgZunqJ+P3D/FPXn6TxtJEkaEj+BLEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJWYRBkt1JDid5sqt2fpL9SZ5rP5e1epLcmmQ8yRNJLu7aZ2sb/1ySrV31n0ry1bbPrUky35OUJJ3abM4MPgVsOql2PfBAVa0FHmjrAJcDa9trO3AbdMID2AFcClwC7DgRIG3M+7v2O/lYkqQFNmMYVNXDwJGTypuBPW15D3BVV/2O6jgALE1yIXAZsL+qjlTVUWA/sKlte0NVHaiqAu7oei9J0oD0es/ggqp6qS2/DFzQllcAB7vGTbTaqeoTU9SnlGR7krEkY5OTkz22Lkk6Wd83kNtv9DUPvczmWLuqarSqRkdGRgZxSElaFHoNg2+0Szy0n4db/RCwqmvcylY7VX3lFHVJ0gD1GgZ7gRNPBG0F7uuqX9ueKloPHGuXk/YBG5MsazeONwL72rZvJVnfniK6tuu9JEkDsmSmAUnuBH4WWJ5kgs5TQb8L3JNkG/B14D1t+P3AFcA48CrwPoCqOpLkJuDRNu7GqjpxU/oDdJ5Y+iHgc+0lSRqgGcOgqq6ZZtOGKcYWcN0077Mb2D1FfQy4aKY+JEkLx08gS5IMA0mSYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEmizzBI8utJnkryZJI7k7w2yZokjyQZT3J3knPb2Ne09fG2fXXX+9zQ6s8muay/KUmS5qrnMEiyAvg3wGhVXQScA2wBPgLcUlVvAo4C29ou24CjrX5LG0eSdW2/twGbgI8nOafXviRJc9fvZaIlwA8lWQK8DngJeBdwb9u+B7iqLW9u67TtG5Kk1e+qqu9U1QvAOHBJn31Jkuag5zCoqkPAfwBepBMCx4DHgFeq6ngbNgGsaMsrgINt3+Nt/Bu761Ps832SbE8ylmRscnKy19YlSSfp5zLRMjq/1a8B/gHww3Qu8yyYqtpVVaNVNToyMrKQh5KkRaWfy0Q/B7xQVZNV9dfAnwDvBJa2y0YAK4FDbfkQsAqgbT8P+GZ3fYp9JEkD0E8YvAisT/K6du1/A/A08BBwdRuzFbivLe9t67TtD1ZVtfqW9rTRGmAt8MU++pIkzdGSmYdMraoeSXIv8CXgOPA4sAv4LHBXkt9ptdvbLrcDn04yDhyh8wQRVfVUknvoBMlx4Lqq+l6vfUmS5q7nMACoqh3AjpPKzzPF00BV9W3gF6Z5n5uBm/vpRZLUOz+BLEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJPsMgydIk9yb58yTPJPnpJOcn2Z/kufZzWRubJLcmGU/yRJKLu95naxv/XJKt/U5KkjQ3/Z4Z/AHwZ1X1FuCfAM8A1wMPVNVa4IG2DnA5sLa9tgO3ASQ5H9gBXApcAuw4ESCSpMHoOQySnAf8DHA7QFV9t6peATYDe9qwPcBVbXkzcEd1HACWJrkQuAzYX1VHquoosB/Y1GtfkqS56+fMYA0wCfxRkseTfDLJDwMXVNVLbczLwAVteQVwsGv/iVabrv73JNmeZCzJ2OTkZB+tS5K69RMGS4CLgduq6h3A/+PvLgkBUFUFVB/H+D5VtauqRqtqdGRkZL7eVpIWvX7CYAKYqKpH2vq9dMLhG+3yD+3n4bb9ELCqa/+VrTZdXZI0ID2HQVW9DBxM8uZW2gA8DewFTjwRtBW4ry3vBa5tTxWtB461y0n7gI1JlrUbxxtbTZI0IEv63P9Xgc8kORd4HngfnYC5J8k24OvAe9rY+4ErgHHg1TaWqjqS5Cbg0Tbuxqo60mdfkqQ56CsMqurLwOgUmzZMMbaA66Z5n93A7n56kST1zk8gS5IMA0mSYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEliHsIgyTlJHk/y39v6miSPJBlPcneSc1v9NW19vG1f3fUeN7T6s0ku67cnSdLczMeZwa8Bz3StfwS4pareBBwFtrX6NuBoq9/SxpFkHbAFeBuwCfh4knPmoS9J0iz1FQZJVgI/D3yyrQd4F3BvG7IHuKotb27rtO0b2vjNwF1V9Z2qegEYBy7ppy9J0tz0e2bwH4EPA3/T1t8IvFJVx9v6BLCiLa8ADgK07cfa+L+tT7HP90myPclYkrHJyck+W5ckndBzGCR5N3C4qh6bx35Oqap2VdVoVY2OjIwM6rCSdNZb0se+7wSuTHIF8FrgDcAfAEuTLGm//a8EDrXxh4BVwESSJcB5wDe76id07yNJGoCezwyq6oaqWllVq+ncAH6wqv4F8BBwdRu2FbivLe9t67TtD1ZVtfqW9rTRGmAt8MVe+5IkzV0/ZwbT+U3griS/AzwO3N7qtwOfTjIOHKETIFTVU0nuAZ4GjgPXVdX3FqAvSdI05iUMqurzwOfb8vNM8TRQVX0b+IVp9r8ZuHk+epEkzZ2fQJYkGQaSJMNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CSRB9hkGRVkoeSPJ3kqSS/1urnJ9mf5Ln2c1mrJ8mtScaTPJHk4q732trGP5dka//TkqQzxM7zAPjoL757qG30c2ZwHPhQVa0D1gPXJVkHXA88UFVrgQfaOsDlwNr22g7cBp3wAHYAlwKXADtOBIgkaTB6DoOqeqmqvtSW/y/wDLAC2AzsacP2AFe15c3AHdVxAFia5ELgMmB/VR2pqqPAfmBTr31JkuZuXu4ZJFkNvAN4BLigql5qm14GLmjLK4CDXbtNtNp09amOsz3JWJKxycnJ+WhdksQ8hEGSHwH+GPi3VfWt7m1VVUD1e4yu99tVVaNVNToyMjJfbytJi15fYZDkB+kEwWeq6k9a+Rvt8g/t5+FWPwSs6tp9ZatNV5ckDUg/TxMFuB14pqp+v2vTXuDEE0Fbgfu66te2p4rWA8fa5aR9wMYky9qN442tJkkakCV97PtO4JeBryb5cqv9FvC7wD1JtgFfB97Ttt0PXAGMA68C7wOoqiNJbgIebeNurKojffQlSZqjnsOgqv4XkGk2b5hifAHXTfNeu4HdvfYiSeqPn0CWJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSTpt7dy5c2DHMgwkSYaBJMkwkCRhGEjSaWX19Z8dynENA0mSYSBJMgwkSRgGkiQMA0kShoEkDd0zb3nrsFs4fcIgyaYkzyYZT3L9sPuRpNPJQgfGaREGSc4BPgZcDqwDrkmybrhdSdLicVqEAXAJMF5Vz1fVd4G7gM1D7kmSFo1U1bB7IMnVwKaq+ldt/ZeBS6vqgyeN2w5sb6tvBp6d46GWA3/VZ7tnosU478U4Z1ic816Mc4be5/3jVTVycnFJ//0MTlXtAnb1un+SsaoanceWzgiLcd6Lcc6wOOe9GOcM8z/v0+Uy0SFgVdf6ylaTJA3A6RIGjwJrk6xJci6wBdg75J4kadE4LS4TVdXxJB8E9gHnALur6qkFOFTPl5jOcItx3otxzrA4570Y5wzzPO/T4gayJGm4TpfLRJKkITIMJElnZxjM9NUWSV6T5O62/ZEkqwff5fybxbx/I8nTSZ5I8kCSHx9Gn/Nptl9jkuSfJ6kkZ8UjiLOZd5L3tD/vp5L8l0H3ON9m8ff7HyZ5KMnj7e/4FcPocz4l2Z3kcJInp9meJLe2fyZPJLm454NV1Vn1onMD+i+BfwScC3wFWHfSmA8An2jLW4C7h933gOb9z4DXteVfOdPnPZs5t3GvBx4GDgCjw+57QH/Wa4HHgWVt/UeH3fcA5rwL+JW2vA742rD7nod5/wxwMfDkNNuvAD4HBFgPPNLrsc7GM4PZfLXFZmBPW74X2JAkA+xxIcw476p6qKpebasH6Hye40w2268xuQn4CPDtQTa3gGYz7/cDH6uqowBVdXjAPc632cy5gDe05fOA/zPA/hZEVT0MHDnFkM3AHdVxAFia5MJejnU2hsEK4GDX+kSrTTmmqo4Dx4A3DqS7hTObeXfbRuc3ijPZjHNup82rqmo4/5fxhTGbP+ufBH4yyf9OciDJpoF1tzBmM+edwC8lmQDuB351MK0N1Vz/vZ/WafE5Aw1Wkl8CRoF/OuxeFlKSHwB+H3jvkFsZhiV0LhX9LJ0zwIeTvL2qXhlqVwvrGuBTVfXRJD8NfDrJRVX1N8Nu7ExwNp4ZzOarLf52TJIldE4pvzmQ7hbOrL7SI8nPAb8NXFlV3xlQbwtlpjm/HrgI+HySr9G5prr3LLiJPJs/6wlgb1X9dVW9APwFnXA4U81mztuAewCq6gvAa+l8mdvZbN6+yudsDIPZfLXFXmBrW74aeLDa3Zgz2IzzTvIO4D/TCYIz/RoyzDDnqjpWVcuranVVraZzn+TKqhobTrvzZjZ/x/8bnbMCkiync9no+UE2Oc9mM+cXgQ0ASd5KJwwmB9rl4O0Frm1PFa0HjlXVS7280Vl3maim+WqLJDcCY1W1F7idzinkOJ2bM1uG1/H8mOW8fw/4EeC/tvvlL1bVlUNruk+znPNZZ5bz3gdsTPI08D3g31XVGXv2O8s5fwj4wyS/Tudm8nvP9F/yktxJJ9SXt3shO4AfBKiqT9C5N3IFMA68Cryv52Od4f+sJEnz4Gy8TCRJmiPDQJJkGEiSDANJEoaBJAnDQJKEYSBJAv4/D5b5Y92GxwcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWicffveSUxd"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def smaller_vgg_net(input_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(units=28, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buioIMZtSd6P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e33e3051-f3b7-4b16-9c32-72599691919f"
      },
      "source": [
        "import time\n",
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "print(timestr)\n",
        "print(images_train[0].shape)\n",
        "print(annotations_train[0])\n",
        "#print(img_shape)\n",
        "img_shape = (60, 160, 3)\n",
        "print(img_shape)\n",
        "\n",
        "\n",
        "timesteps = 1\n",
        "batch_size = 50  # 16\n",
        "nb_epoch = 100  # 223\n",
        "model_name = 'smaller_vgg_net'\n",
        "model = smaller_vgg_net(img_shape)\n",
        "model_filename = timestr + '_smaller_vgg_net_model_300_RGB_extreme_sequence'\n",
        "\n",
        "\n",
        "if type_image == 'cropped':\n",
        "    model_file = '/content/drive/My Drive/' + model_filename + '.h5'\n",
        "\n",
        "\n",
        "# Print layers\n",
        "print(model)\n",
        "model.build(img_shape)\n",
        "print(model.summary())\n",
        "\n",
        "import datetime\n",
        "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, CSVLogger\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "earlystopping=EarlyStopping(monitor=\"accuracy\", patience=40, verbose=1, mode='auto')\n",
        "# Create a callback that saves the model's weights\n",
        "checkpoint_path = \"/content/drive/My Drive/\" + model_filename + '_cp.h5'\n",
        "cp_callback = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, verbose=1)\n",
        "csv_logger = CSVLogger(\"/content/drive/My Drive/\" + model_filename + '.csv', append=True)\n",
        "\n",
        "\n",
        "# Train\n",
        "model_history = model.fit(images_train, classes_train, epochs=nb_epoch, batch_size=batch_size, verbose=2, validation_data=(images_validation, classes_validation), callbacks=[tensorboard_callback, earlystopping, cp_callback, csv_logger])\n",
        "\n",
        "# Save the model\n",
        "model.save(model_file)\n",
        "\n",
        "\n",
        "# Evaluate the modelpil\n",
        "score = model.evaluate(images_validation, annotations_validation, verbose=0)\n",
        "\n",
        "print('Evaluating')\n",
        "print('Test loss: ', score[0])\n",
        "print('Test mean squared error: ', score[1])\n",
        "print('Test mean absolute error: ', score[2])\n",
        "\n",
        "\n",
        "# SAVE METADATA\n",
        "from tensorflow.python.keras.saving import hdf5_format\n",
        "import h5py\n",
        "\n",
        "model_path = model_file\n",
        "# Save model\n",
        "with h5py.File(model_path, mode='w') as f:\n",
        "    hdf5_format.save_model_to_hdf5(model, f)\n",
        "    f.attrs['experiment_name'] = ''\n",
        "    f.attrs['experiment_description'] = ''\n",
        "    f.attrs['batch_size'] = batch_size\n",
        "    f.attrs['nb_epoch'] = nb_epoch\n",
        "    f.attrs['model'] = model_name\n",
        "    f.attrs['img_shape'] = img_shape\n",
        "    f.attrs['normalized_dataset'] = True\n",
        "    f.attrs['sequences_dataset'] = True\n",
        "    f.attrs['gpu_trained'] = True\n",
        "    f.attrs['data_augmentation'] = True\n",
        "    f.attrs['extreme_data'] = False\n",
        "    f.attrs['split_test_train'] = 0.30\n",
        "    f.attrs['instances_number'] = len(array_annotations)\n",
        "    f.attrs['loss'] = score[0]\n",
        "    f.attrs['mse'] = score[1]\n",
        "    f.attrs['mae'] = score[2]\n",
        "    f.attrs['csv_path'] = model_filename + '.csv'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20210304-182418\n",
            "(60, 160, 3)\n",
            "['slow' 'slightly_left']\n",
            "(60, 160, 3)\n",
            "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fd8dc776d50>\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_35 (Conv2D)           (None, 60, 160, 32)       896       \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 60, 160, 32)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 60, 160, 32)       128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 20, 53, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 20, 53, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 20, 53, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 20, 53, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 20, 53, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 20, 53, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 20, 53, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 20, 53, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 10, 26, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 10, 26, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 10, 26, 128)       73856     \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 10, 26, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 10, 26, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 10, 26, 128)       147584    \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 10, 26, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 10, 26, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 5, 13, 128)        0         \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 5, 13, 128)        0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 8320)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1024)              8520704   \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 28)                28700     \n",
            "=================================================================\n",
            "Total params: 8,832,924\n",
            "Trainable params: 8,830,044\n",
            "Non-trainable params: 2,880\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "692/692 - 41s - loss: 0.8586 - accuracy: 0.7394 - val_loss: 0.5382 - val_accuracy: 0.8154\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.53821, saving model to /content/drive/My Drive/20210304-182418_smaller_vgg_net_model_300_RGB_extreme_sequence_cp.h5\n",
            "Epoch 2/100\n",
            "692/692 - 39s - loss: 0.4846 - accuracy: 0.8324 - val_loss: 0.3812 - val_accuracy: 0.8666\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.53821 to 0.38119, saving model to /content/drive/My Drive/20210304-182418_smaller_vgg_net_model_300_RGB_extreme_sequence_cp.h5\n",
            "Epoch 3/100\n",
            "692/692 - 38s - loss: 0.4193 - accuracy: 0.8544 - val_loss: 0.3357 - val_accuracy: 0.8830\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.38119 to 0.33568, saving model to /content/drive/My Drive/20210304-182418_smaller_vgg_net_model_300_RGB_extreme_sequence_cp.h5\n",
            "Epoch 4/100\n",
            "692/692 - 39s - loss: 0.3591 - accuracy: 0.8757 - val_loss: 0.2885 - val_accuracy: 0.8961\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.33568 to 0.28845, saving model to /content/drive/My Drive/20210304-182418_smaller_vgg_net_model_300_RGB_extreme_sequence_cp.h5\n",
            "Epoch 5/100\n",
            "692/692 - 38s - loss: 0.3280 - accuracy: 0.8833 - val_loss: 0.3062 - val_accuracy: 0.8962\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.28845\n",
            "Epoch 6/100\n",
            "692/692 - 39s - loss: 0.3270 - accuracy: 0.8838 - val_loss: 0.2991 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.28845\n",
            "Epoch 7/100\n",
            "692/692 - 38s - loss: 0.3026 - accuracy: 0.8953 - val_loss: 0.2915 - val_accuracy: 0.8988\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.28845\n",
            "Epoch 8/100\n",
            "692/692 - 39s - loss: 0.2897 - accuracy: 0.8979 - val_loss: 0.2554 - val_accuracy: 0.9106\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.28845 to 0.25543, saving model to /content/drive/My Drive/20210304-182418_smaller_vgg_net_model_300_RGB_extreme_sequence_cp.h5\n",
            "Epoch 9/100\n",
            "692/692 - 38s - loss: 0.2667 - accuracy: 0.9059 - val_loss: 0.3123 - val_accuracy: 0.9000\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.25543\n",
            "Epoch 10/100\n",
            "692/692 - 39s - loss: 0.2720 - accuracy: 0.9042 - val_loss: 0.2389 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.25543 to 0.23889, saving model to /content/drive/My Drive/20210304-182418_smaller_vgg_net_model_300_RGB_extreme_sequence_cp.h5\n",
            "Epoch 11/100\n",
            "692/692 - 38s - loss: 0.2451 - accuracy: 0.9133 - val_loss: 0.2329 - val_accuracy: 0.9183\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.23889 to 0.23290, saving model to /content/drive/My Drive/20210304-182418_smaller_vgg_net_model_300_RGB_extreme_sequence_cp.h5\n",
            "Epoch 12/100\n",
            "692/692 - 39s - loss: 0.2372 - accuracy: 0.9180 - val_loss: 0.2635 - val_accuracy: 0.9141\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.23290\n",
            "Epoch 13/100\n",
            "692/692 - 38s - loss: 0.2273 - accuracy: 0.9184 - val_loss: 0.2625 - val_accuracy: 0.9154\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.23290\n",
            "Epoch 14/100\n",
            "692/692 - 38s - loss: 0.2020 - accuracy: 0.9286 - val_loss: 0.2155 - val_accuracy: 0.9276\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.23290 to 0.21552, saving model to /content/drive/My Drive/20210304-182418_smaller_vgg_net_model_300_RGB_extreme_sequence_cp.h5\n",
            "Epoch 15/100\n",
            "692/692 - 38s - loss: 0.1980 - accuracy: 0.9295 - val_loss: 0.2332 - val_accuracy: 0.9239\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.21552\n",
            "Epoch 16/100\n",
            "692/692 - 38s - loss: 0.1884 - accuracy: 0.9319 - val_loss: 0.2378 - val_accuracy: 0.9247\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.21552\n",
            "Epoch 17/100\n",
            "692/692 - 38s - loss: 0.1882 - accuracy: 0.9335 - val_loss: 0.2365 - val_accuracy: 0.9265\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.21552\n",
            "Epoch 18/100\n",
            "692/692 - 38s - loss: 0.1755 - accuracy: 0.9363 - val_loss: 0.2333 - val_accuracy: 0.9235\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.21552\n",
            "Epoch 19/100\n",
            "692/692 - 38s - loss: 0.1625 - accuracy: 0.9414 - val_loss: 0.2262 - val_accuracy: 0.9289\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.21552\n",
            "Epoch 20/100\n",
            "692/692 - 39s - loss: 0.1508 - accuracy: 0.9456 - val_loss: 1.0264 - val_accuracy: 0.8721\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.21552\n",
            "Epoch 21/100\n",
            "692/692 - 38s - loss: 0.1421 - accuracy: 0.9478 - val_loss: 0.2273 - val_accuracy: 0.9292\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.21552\n",
            "Epoch 22/100\n",
            "692/692 - 38s - loss: 0.1365 - accuracy: 0.9523 - val_loss: 1.0468 - val_accuracy: 0.8942\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.21552\n",
            "Epoch 23/100\n",
            "692/692 - 38s - loss: 0.1318 - accuracy: 0.9530 - val_loss: 0.2532 - val_accuracy: 0.9290\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.21552\n",
            "Epoch 24/100\n",
            "692/692 - 38s - loss: 0.1234 - accuracy: 0.9565 - val_loss: 0.2453 - val_accuracy: 0.9295\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.21552\n",
            "Epoch 25/100\n",
            "692/692 - 38s - loss: 0.1254 - accuracy: 0.9551 - val_loss: 0.3943 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.21552\n",
            "Epoch 26/100\n",
            "692/692 - 39s - loss: 0.1246 - accuracy: 0.9558 - val_loss: 0.2358 - val_accuracy: 0.9316\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.21552\n",
            "Epoch 27/100\n",
            "692/692 - 38s - loss: 0.1095 - accuracy: 0.9611 - val_loss: 0.2657 - val_accuracy: 0.9297\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.21552\n",
            "Epoch 28/100\n",
            "692/692 - 38s - loss: 0.1019 - accuracy: 0.9628 - val_loss: 1.4154 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.21552\n",
            "Epoch 29/100\n",
            "692/692 - 38s - loss: 0.1004 - accuracy: 0.9650 - val_loss: 0.2736 - val_accuracy: 0.9304\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.21552\n",
            "Epoch 30/100\n",
            "692/692 - 38s - loss: 0.0994 - accuracy: 0.9645 - val_loss: 0.2850 - val_accuracy: 0.9308\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.21552\n",
            "Epoch 31/100\n",
            "692/692 - 38s - loss: 0.0951 - accuracy: 0.9668 - val_loss: 0.2656 - val_accuracy: 0.9316\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.21552\n",
            "Epoch 32/100\n",
            "692/692 - 38s - loss: 0.0880 - accuracy: 0.9682 - val_loss: 0.2681 - val_accuracy: 0.9318\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.21552\n",
            "Epoch 33/100\n",
            "692/692 - 38s - loss: 0.0833 - accuracy: 0.9714 - val_loss: 0.2694 - val_accuracy: 0.9338\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.21552\n",
            "Epoch 34/100\n",
            "692/692 - 38s - loss: 0.0834 - accuracy: 0.9702 - val_loss: 0.2574 - val_accuracy: 0.9356\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.21552\n",
            "Epoch 35/100\n",
            "692/692 - 38s - loss: 0.0749 - accuracy: 0.9736 - val_loss: 0.2839 - val_accuracy: 0.9334\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.21552\n",
            "Epoch 36/100\n",
            "692/692 - 38s - loss: 0.0919 - accuracy: 0.9682 - val_loss: 0.2592 - val_accuracy: 0.9336\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.21552\n",
            "Epoch 37/100\n",
            "692/692 - 38s - loss: 0.0729 - accuracy: 0.9748 - val_loss: 0.2847 - val_accuracy: 0.9336\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.21552\n",
            "Epoch 38/100\n",
            "692/692 - 38s - loss: 0.0776 - accuracy: 0.9734 - val_loss: 0.2695 - val_accuracy: 0.9331\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.21552\n",
            "Epoch 39/100\n",
            "692/692 - 38s - loss: 0.0676 - accuracy: 0.9754 - val_loss: 0.2867 - val_accuracy: 0.9363\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.21552\n",
            "Epoch 40/100\n",
            "692/692 - 38s - loss: 0.0662 - accuracy: 0.9771 - val_loss: 0.2956 - val_accuracy: 0.9334\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.21552\n",
            "Epoch 41/100\n",
            "692/692 - 38s - loss: 0.0641 - accuracy: 0.9778 - val_loss: 0.2928 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.21552\n",
            "Epoch 42/100\n",
            "692/692 - 38s - loss: 0.0604 - accuracy: 0.9791 - val_loss: 0.3017 - val_accuracy: 0.9316\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.21552\n",
            "Epoch 43/100\n",
            "692/692 - 38s - loss: 0.0660 - accuracy: 0.9774 - val_loss: 0.2847 - val_accuracy: 0.9336\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.21552\n",
            "Epoch 44/100\n",
            "692/692 - 38s - loss: 0.0611 - accuracy: 0.9791 - val_loss: 0.3106 - val_accuracy: 0.9328\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.21552\n",
            "Epoch 45/100\n",
            "692/692 - 38s - loss: 0.0559 - accuracy: 0.9800 - val_loss: 0.3091 - val_accuracy: 0.9336\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.21552\n",
            "Epoch 46/100\n",
            "692/692 - 38s - loss: 0.0567 - accuracy: 0.9802 - val_loss: 0.2844 - val_accuracy: 0.9372\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.21552\n",
            "Epoch 47/100\n",
            "692/692 - 38s - loss: 0.0553 - accuracy: 0.9813 - val_loss: 0.3072 - val_accuracy: 0.9343\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.21552\n",
            "Epoch 48/100\n",
            "692/692 - 38s - loss: 0.0537 - accuracy: 0.9813 - val_loss: 0.2856 - val_accuracy: 0.9368\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.21552\n",
            "Epoch 49/100\n",
            "692/692 - 38s - loss: 0.0581 - accuracy: 0.9801 - val_loss: 0.3129 - val_accuracy: 0.9339\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.21552\n",
            "Epoch 50/100\n",
            "692/692 - 38s - loss: 0.0529 - accuracy: 0.9822 - val_loss: 0.3092 - val_accuracy: 0.9339\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.21552\n",
            "Epoch 51/100\n",
            "692/692 - 38s - loss: 0.0526 - accuracy: 0.9824 - val_loss: 0.3245 - val_accuracy: 0.9338\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.21552\n",
            "Epoch 52/100\n",
            "692/692 - 38s - loss: 0.0478 - accuracy: 0.9831 - val_loss: 0.3132 - val_accuracy: 0.9346\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.21552\n",
            "Epoch 53/100\n",
            "692/692 - 38s - loss: 0.0588 - accuracy: 0.9800 - val_loss: 0.3301 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.21552\n",
            "Epoch 54/100\n",
            "692/692 - 38s - loss: 0.0536 - accuracy: 0.9812 - val_loss: 0.3265 - val_accuracy: 0.9323\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.21552\n",
            "Epoch 55/100\n",
            "692/692 - 38s - loss: 0.0445 - accuracy: 0.9843 - val_loss: 0.3293 - val_accuracy: 0.9321\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.21552\n",
            "Epoch 56/100\n",
            "692/692 - 38s - loss: 0.0485 - accuracy: 0.9834 - val_loss: 0.3183 - val_accuracy: 0.9336\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.21552\n",
            "Epoch 57/100\n",
            "692/692 - 38s - loss: 0.0484 - accuracy: 0.9837 - val_loss: 0.3647 - val_accuracy: 0.9334\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.21552\n",
            "Epoch 58/100\n",
            "692/692 - 38s - loss: 0.0430 - accuracy: 0.9849 - val_loss: 0.3459 - val_accuracy: 0.9350\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.21552\n",
            "Epoch 59/100\n",
            "692/692 - 38s - loss: 0.0393 - accuracy: 0.9865 - val_loss: 0.3240 - val_accuracy: 0.9375\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.21552\n",
            "Epoch 60/100\n",
            "692/692 - 38s - loss: 0.0428 - accuracy: 0.9856 - val_loss: 0.3309 - val_accuracy: 0.9344\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.21552\n",
            "Epoch 61/100\n",
            "692/692 - 38s - loss: 0.0440 - accuracy: 0.9850 - val_loss: 0.3216 - val_accuracy: 0.9359\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.21552\n",
            "Epoch 62/100\n",
            "692/692 - 38s - loss: 0.0405 - accuracy: 0.9867 - val_loss: 0.3596 - val_accuracy: 0.9332\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.21552\n",
            "Epoch 63/100\n",
            "692/692 - 38s - loss: 0.0420 - accuracy: 0.9861 - val_loss: 0.3532 - val_accuracy: 0.9335\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.21552\n",
            "Epoch 64/100\n",
            "692/692 - 38s - loss: 0.0435 - accuracy: 0.9851 - val_loss: 0.3589 - val_accuracy: 0.9347\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.21552\n",
            "Epoch 65/100\n",
            "692/692 - 38s - loss: 0.0389 - accuracy: 0.9867 - val_loss: 0.3589 - val_accuracy: 0.9353\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.21552\n",
            "Epoch 66/100\n",
            "692/692 - 38s - loss: 0.0409 - accuracy: 0.9870 - val_loss: 0.3433 - val_accuracy: 0.9334\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.21552\n",
            "Epoch 67/100\n",
            "692/692 - 38s - loss: 0.0370 - accuracy: 0.9876 - val_loss: 0.3720 - val_accuracy: 0.9336\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.21552\n",
            "Epoch 68/100\n",
            "692/692 - 38s - loss: 0.0425 - accuracy: 0.9859 - val_loss: 0.3381 - val_accuracy: 0.9338\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.21552\n",
            "Epoch 69/100\n",
            "692/692 - 38s - loss: 0.0361 - accuracy: 0.9879 - val_loss: 0.3449 - val_accuracy: 0.9343\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.21552\n",
            "Epoch 70/100\n",
            "692/692 - 38s - loss: 0.0341 - accuracy: 0.9883 - val_loss: 0.3841 - val_accuracy: 0.9342\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.21552\n",
            "Epoch 71/100\n",
            "692/692 - 38s - loss: 0.0363 - accuracy: 0.9880 - val_loss: 0.3692 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.21552\n",
            "Epoch 72/100\n",
            "692/692 - 38s - loss: 0.0376 - accuracy: 0.9878 - val_loss: 0.3651 - val_accuracy: 0.9358\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.21552\n",
            "Epoch 73/100\n",
            "692/692 - 38s - loss: 0.0412 - accuracy: 0.9865 - val_loss: 0.3617 - val_accuracy: 0.9338\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.21552\n",
            "Epoch 74/100\n",
            "692/692 - 38s - loss: 0.0378 - accuracy: 0.9876 - val_loss: 0.3879 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.21552\n",
            "Epoch 75/100\n",
            "692/692 - 38s - loss: 0.0327 - accuracy: 0.9894 - val_loss: 0.3472 - val_accuracy: 0.9328\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.21552\n",
            "Epoch 76/100\n",
            "692/692 - 38s - loss: 0.0289 - accuracy: 0.9902 - val_loss: 0.3838 - val_accuracy: 0.9356\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.21552\n",
            "Epoch 77/100\n",
            "692/692 - 38s - loss: 0.0317 - accuracy: 0.9896 - val_loss: 0.3906 - val_accuracy: 0.9351\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.21552\n",
            "Epoch 78/100\n",
            "692/692 - 38s - loss: 0.0345 - accuracy: 0.9889 - val_loss: 0.3618 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.21552\n",
            "Epoch 79/100\n",
            "692/692 - 38s - loss: 0.0344 - accuracy: 0.9891 - val_loss: 0.4048 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.21552\n",
            "Epoch 80/100\n",
            "692/692 - 38s - loss: 0.0310 - accuracy: 0.9900 - val_loss: 0.3753 - val_accuracy: 0.9352\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.21552\n",
            "Epoch 81/100\n",
            "692/692 - 38s - loss: 0.0294 - accuracy: 0.9907 - val_loss: 0.4167 - val_accuracy: 0.9370\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.21552\n",
            "Epoch 82/100\n",
            "692/692 - 38s - loss: 0.0302 - accuracy: 0.9901 - val_loss: 0.3940 - val_accuracy: 0.9346\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.21552\n",
            "Epoch 83/100\n",
            "692/692 - 38s - loss: 0.0358 - accuracy: 0.9881 - val_loss: 0.3845 - val_accuracy: 0.9328\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.21552\n",
            "Epoch 84/100\n",
            "692/692 - 38s - loss: 0.0308 - accuracy: 0.9898 - val_loss: 0.4086 - val_accuracy: 0.9349\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.21552\n",
            "Epoch 85/100\n",
            "692/692 - 38s - loss: 0.0306 - accuracy: 0.9907 - val_loss: 0.3539 - val_accuracy: 0.9349\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.21552\n",
            "Epoch 86/100\n",
            "692/692 - 38s - loss: 0.0296 - accuracy: 0.9910 - val_loss: 0.3924 - val_accuracy: 0.9345\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.21552\n",
            "Epoch 87/100\n",
            "692/692 - 38s - loss: 0.0291 - accuracy: 0.9905 - val_loss: 0.3884 - val_accuracy: 0.9352\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.21552\n",
            "Epoch 88/100\n",
            "692/692 - 38s - loss: 0.0361 - accuracy: 0.9889 - val_loss: 0.4125 - val_accuracy: 0.9339\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.21552\n",
            "Epoch 89/100\n",
            "692/692 - 38s - loss: 0.0286 - accuracy: 0.9902 - val_loss: 0.4063 - val_accuracy: 0.9357\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.21552\n",
            "Epoch 90/100\n",
            "692/692 - 38s - loss: 0.0281 - accuracy: 0.9909 - val_loss: 0.4040 - val_accuracy: 0.9351\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.21552\n",
            "Epoch 91/100\n",
            "692/692 - 38s - loss: 0.0263 - accuracy: 0.9916 - val_loss: 0.4305 - val_accuracy: 0.9350\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.21552\n",
            "Epoch 92/100\n",
            "692/692 - 38s - loss: 0.0278 - accuracy: 0.9911 - val_loss: 0.4178 - val_accuracy: 0.9354\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.21552\n",
            "Epoch 93/100\n",
            "692/692 - 38s - loss: 0.0236 - accuracy: 0.9927 - val_loss: 0.3725 - val_accuracy: 0.9341\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.21552\n",
            "Epoch 94/100\n",
            "692/692 - 38s - loss: 0.0292 - accuracy: 0.9905 - val_loss: 0.3982 - val_accuracy: 0.9355\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.21552\n",
            "Epoch 95/100\n",
            "692/692 - 38s - loss: 0.0257 - accuracy: 0.9920 - val_loss: 0.3862 - val_accuracy: 0.9358\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.21552\n",
            "Epoch 96/100\n",
            "692/692 - 38s - loss: 0.0263 - accuracy: 0.9919 - val_loss: 0.4373 - val_accuracy: 0.9351\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.21552\n",
            "Epoch 97/100\n",
            "692/692 - 38s - loss: 0.0269 - accuracy: 0.9912 - val_loss: 0.3979 - val_accuracy: 0.9353\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.21552\n",
            "Epoch 98/100\n",
            "692/692 - 38s - loss: 0.0287 - accuracy: 0.9910 - val_loss: 0.4381 - val_accuracy: 0.9316\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.21552\n",
            "Epoch 99/100\n",
            "692/692 - 38s - loss: 0.0285 - accuracy: 0.9911 - val_loss: 0.4187 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.21552\n",
            "Epoch 100/100\n",
            "692/692 - 38s - loss: 0.0295 - accuracy: 0.9907 - val_loss: 0.3888 - val_accuracy: 0.9336\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.21552\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-e2872a701223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Evaluate the modelpil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Evaluating'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m-> 2941\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3357\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3358\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3280\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1233 test_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1224 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1217 run_step  **\n        outputs = model.test_step(data)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1186 test_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:1537 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4833 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 2) and (None, 28) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKQjwTMpKL-C"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}