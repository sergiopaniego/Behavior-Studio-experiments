{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BS_lenet_classification_network_extreme_dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3Jz7OQlMWR8rmZTLW7cVB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiopaniego/BehaviorStudio-experiments/blob/main/BS_lenet_classification_network_extreme_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQU41fYJSAhy",
        "outputId": "792bbfde-c18a-4319-efda-efc968203348"
      },
      "source": [
        "# When using Colab, check the GPU that is assigned and reload the runtime if its memory is low\n",
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Mar 16 13:36:29 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ibWmo-WSMmH",
        "outputId": "c592e2ed-86ae-4b79-9c25-bc40d4984533"
      },
      "source": [
        "# Mount Google Drive to access images dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egUAtW3FSPk1",
        "outputId": "48658c51-0cf2-48d8-8e46-1739a95f9886"
      },
      "source": [
        "!ls \"/content/drive/My Drive\"\n",
        "!ls \"/content/drive/My Drive/complete_dataset.zip\"\n",
        "!unzip \"/content/drive/My Drive/curves_only.zip\"\n",
        "!unzip \"/content/drive/My Drive/complete_dataset.zip\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 20210215-134322_model_lstm_sequence_test_cp.h5\n",
            " 20210215-134322_model_lstm_sequence_test.csv\n",
            " 20210215-134625_model_lstm_sequence_test_cp.h5\n",
            " 20210215-134625_model_lstm_sequence_test.csv\n",
            " 20210215-134625_model_lstm_sequence_test.h5\n",
            " 20210215-180738_deepest_lstm_tinypilotnet_model_sequence_cp.h5\n",
            " 20210215-180738_deepest_lstm_tinypilotnet_model_sequence.csv\n",
            " 20210215-180738_deepest_lstm_tinypilotnet_model_sequence.h5\n",
            " 20210216-084753_tinypilotnet_lstm_model_sequence_cp.h5\n",
            " 20210216-084753_tinypilotnet_lstm_model_sequence.csv\n",
            " 20210216-084753_tinypilotnet_lstm_model_sequence.h5\n",
            " 20210216-091101_tinypilotnet_model_sequence_cp.h5\n",
            " 20210216-091101_tinypilotnet_model_sequence.csv\n",
            " 20210216-091101_tinypilotnet_model_sequence.h5\n",
            " 20210216-095459_pilotnet_model_model_sequence_cp.h5\n",
            " 20210216-095459_pilotnet_model_model_sequence.csv\n",
            " 20210216-095459_pilotnet_model_model_sequence.h5\n",
            " 20210219-082603_deepest_lstm_tinypilotnet_model_RGB_sequence_cp.h5\n",
            " 20210219-082603_deepest_lstm_tinypilotnet_model_RGB_sequence.csv\n",
            " 20210219-082603_deepest_lstm_tinypilotnet_model_RGB_sequence.h5\n",
            " 20210219-164202_pilotnet_model_model_RGB_sequence_cp.h5\n",
            " 20210219-164202_pilotnet_model_model_RGB_sequence.csv\n",
            " 20210219-164202_pilotnet_model_model_RGB_sequence.h5\n",
            " 20210219-164420_tinypilotnet_lstm_model_RGB_sequence.csv\n",
            " 20210219-164446_tinypilotnet_model_RGB_sequence_cp.h5\n",
            " 20210219-164446_tinypilotnet_model_RGB_sequence.csv\n",
            " 20210219-164446_tinypilotnet_model_RGB_sequence.h5\n",
            " 20210219-171505_tinypilotnet_lstm_model_RGB_sequence_cp.h5\n",
            " 20210219-171505_tinypilotnet_lstm_model_RGB_sequence.csv\n",
            " 20210219-171505_tinypilotnet_lstm_model_RGB_sequence.h5\n",
            " 20210222-145945_deepest_lstm_tinypilotnet_model_RGB_sequence.csv\n",
            " 20210222-150042_deepest_lstm_tinypilotnet_model_RGB_sequence_cp.h5\n",
            " 20210222-150042_deepest_lstm_tinypilotnet_model_RGB_sequence.csv\n",
            " 20210222-150042_deepest_lstm_tinypilotnet_model_RGB_sequence.h5\n",
            " 20210222-160448_deepest_lstm_tinypilotnet_model_300_RGB_sequence_cp.h5\n",
            " 20210222-160448_deepest_lstm_tinypilotnet_model_300_RGB_sequence.csv\n",
            " 20210222-161338_tinypilotnet_lstm_model_300_RGB_sequence_cp.h5\n",
            " 20210222-161338_tinypilotnet_lstm_model_300_RGB_sequence.csv\n",
            " 20210222-161338_tinypilotnet_lstm_model_300_RGB_sequence.h5\n",
            " 20210222-173243_tinypilotnet_model_300_RGB_sequence_cp.h5\n",
            " 20210222-173243_tinypilotnet_model_300_RGB_sequence.csv\n",
            " 20210222-173243_tinypilotnet_model_300_RGB_sequence.h5\n",
            " 20210222-175955_pilotnet_model_model_300_RGB_sequence_cp.h5\n",
            " 20210222-175955_pilotnet_model_model_300_RGB_sequence.csv\n",
            " 20210222-175955_pilotnet_model_model_300_RGB_sequence.h5\n",
            " 20210222-183328_deepest_lstm_tinypilotnet_model_300_RGB_sequence_cp.h5\n",
            " 20210222-183328_deepest_lstm_tinypilotnet_model_300_RGB_sequence.csv\n",
            " 20210222-183328_deepest_lstm_tinypilotnet_model_300_RGB_sequence.h5\n",
            " 20210224-094211_tinypilotnet_model_25_RGB_sequence_cp.h5\n",
            " 20210224-094211_tinypilotnet_model_25_RGB_sequence.csv\n",
            " 20210224-094211_tinypilotnet_model_25_RGB_sequence.h5\n",
            " 20210224-100616_tinypilotnet_model_50_RGB_sequence.csv\n",
            " 20210224-101345_tinypilotnet_model_50_RGB_sequence_cp.h5\n",
            " 20210224-101345_tinypilotnet_model_50_RGB_sequence.csv\n",
            " 20210224-101345_tinypilotnet_model_50_RGB_sequence.h5\n",
            " 20210224-101937_tinypilotnet_model_50_RGB_sequence_cp.h5\n",
            " 20210224-101937_tinypilotnet_model_50_RGB_sequence.csv\n",
            " 20210224-101937_tinypilotnet_model_50_RGB_sequence.h5\n",
            " 20210224-102723_tinypilotnet_model_300_RGB_sequence_cp.h5\n",
            " 20210224-102723_tinypilotnet_model_300_RGB_sequence.csv\n",
            " 20210224-102723_tinypilotnet_model_300_RGB_sequence.h5\n",
            " 20210224-110142_tinypilotnet_model_300_RGB_sequence_cp.h5\n",
            " 20210224-110142_tinypilotnet_model_300_RGB_sequence.csv\n",
            " 20210224-110142_tinypilotnet_model_300_RGB_sequence.h5\n",
            " 20210226-100325_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch_cp.h5\n",
            " 20210226-100325_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch.csv\n",
            " 20210226-100325_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch.h5\n",
            " 20210226-103100_tinypilotnet_model_300_RGB_sequence_extreme_cp.h5\n",
            " 20210226-103100_tinypilotnet_model_300_RGB_sequence_extreme.csv\n",
            " 20210226-103100_tinypilotnet_model_300_RGB_sequence_extreme.h5\n",
            " 20210301-153401_pilotnet_model_model_300_RGB_extreme_sequence_cp.h5\n",
            " 20210301-153401_pilotnet_model_model_300_RGB_extreme_sequence.csv\n",
            " 20210301-153401_pilotnet_model_model_300_RGB_extreme_sequence.h5\n",
            " 20210301-172225_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch_cp.h5\n",
            " 20210301-172225_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch.csv\n",
            " 20210301-172225_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch.h5\n",
            " 20210301-174942_tinypilotnet_lstm_model_300_RGB_sequence_cp.h5\n",
            " 20210301-174942_tinypilotnet_lstm_model_300_RGB_sequence.csv\n",
            " 20210301-174942_tinypilotnet_lstm_model_300_RGB_sequence.h5\n",
            " 20210302-082158_tinypilotnet_lstm_model_300_RGB_sequence_cp.h5\n",
            " 20210302-082158_tinypilotnet_lstm_model_300_RGB_sequence.csv\n",
            " 20210302-082158_tinypilotnet_lstm_model_300_RGB_sequence.h5\n",
            " 20210302-082208_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch_cp.h5\n",
            " 20210302-082208_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch.csv\n",
            " 20210302-082208_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch.h5\n",
            " 20210302-121834_deepest_lstm_tinypilotnet_model_50_RGB_sequence_cp.h5\n",
            " 20210302-121834_deepest_lstm_tinypilotnet_model_50_RGB_sequence.csv\n",
            " 20210302-121834_deepest_lstm_tinypilotnet_model_50_RGB_sequence.h5\n",
            " 20210302-123811_deepest_lstm_tinypilotnet_model_300_RGB_sequence_cp.h5\n",
            " 20210302-123811_deepest_lstm_tinypilotnet_model_300_RGB_sequence.csv\n",
            " 20210302-123811_deepest_lstm_tinypilotnet_model_300_RGB_sequence.h5\n",
            " 20210304-182418_smaller_vgg_net_model_300_RGB_extreme_sequence_cp.h5\n",
            " 20210304-182418_smaller_vgg_net_model_300_RGB_extreme_sequence.csv\n",
            " 20210304-182418_smaller_vgg_net_model_300_RGB_extreme_sequence.h5\n",
            " 20210309-095328_tinypilotnet_lstm_model_300_RGB_sequence_extreme_cp.h5\n",
            " 20210309-095328_tinypilotnet_lstm_model_300_RGB_sequence_extreme.csv\n",
            " 20210309-095328_tinypilotnet_lstm_model_300_RGB_sequence_extreme.h5\n",
            " 20210310-111153_deepest_lstm_tinypilotnet_model_300_RGB_sequence_no_moderate_cp.h5\n",
            " 20210310-111153_deepest_lstm_tinypilotnet_model_300_RGB_sequence_no_moderate.csv\n",
            " 20210310-111153_deepest_lstm_tinypilotnet_model_300_RGB_sequence_no_moderate.h5\n",
            " 20210310-120747_deepest_lstm_tinypilotnet_model_300_RGB_sequence_extreme_cp.h5\n",
            " 20210310-120747_deepest_lstm_tinypilotnet_model_300_RGB_sequence_extreme.csv\n",
            " 20210310-120747_deepest_lstm_tinypilotnet_model_300_RGB_sequence_extreme.h5\n",
            " 20210310-150111_deepest_lstm_tinypilotnet_model_300_RGB_sequence_extreme_no_moderate_cp.h5\n",
            " 20210310-150111_deepest_lstm_tinypilotnet_model_300_RGB_sequence_extreme_no_moderate.csv\n",
            " 20210310-150111_deepest_lstm_tinypilotnet_model_300_RGB_sequence_extreme_no_moderate.h5\n",
            " 20210310-164008_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch_augmentation_cp.h5\n",
            " 20210310-164008_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch_augmentation.csv\n",
            " 20210310-165143_deepest_lstm_tinypilotnet_model_20_RGB_sequence_patch_augmentation_cp.h5\n",
            " 20210310-165143_deepest_lstm_tinypilotnet_model_20_RGB_sequence_patch_augmentation.csv\n",
            " 20210310-165143_deepest_lstm_tinypilotnet_model_20_RGB_sequence_patch_augmentation.h5\n",
            " 20210311-081347_deepest_lstm_tinypilotnet_model_50_RGB_sequence_patch_augmentation_cp.h5\n",
            " 20210311-081347_deepest_lstm_tinypilotnet_model_50_RGB_sequence_patch_augmentation.csv\n",
            " 20210311-081347_deepest_lstm_tinypilotnet_model_50_RGB_sequence_patch_augmentation.h5\n",
            " 20210311-120817_deepest_lstm_tinypilotnet_model_75_RGB_sequence_patch_augmentation_cp.h5\n",
            " 20210311-120817_deepest_lstm_tinypilotnet_model_75_RGB_sequence_patch_augmentation.csv\n",
            " 20210311-120817_deepest_lstm_tinypilotnet_model_75_RGB_sequence_patch_augmentation.h5\n",
            " 20210311-153739_deepest_lstm_tinypilotnet_model_100_RGB_sequence_patch_augmentation_cp.h5\n",
            " 20210311-153739_deepest_lstm_tinypilotnet_model_100_RGB_sequence_patch_augmentation.csv\n",
            " 20210311-153739_deepest_lstm_tinypilotnet_model_100_RGB_sequence_patch_augmentation.h5\n",
            " 20210312-083735_deepest_lstm_tinypilotnet_model_150_RGB_sequence_patch_augmentation_cp.h5\n",
            " 20210312-083735_deepest_lstm_tinypilotnet_model_150_RGB_sequence_patch_augmentation.csv\n",
            " 20210312-083735_deepest_lstm_tinypilotnet_model_150_RGB_sequence_patch_augmentation.h5\n",
            " 20210312-115032_deepest_lstm_tinypilotnet_model_150_RGB_sequence_patch_augmentation_cp.h5\n",
            " 20210312-115032_deepest_lstm_tinypilotnet_model_150_RGB_sequence_patch_augmentation.csv\n",
            " 20210312-115032_deepest_lstm_tinypilotnet_model_150_RGB_sequence_patch_augmentation.h5\n",
            " 20210312-144611_deepest_lstm_tinypilotnet_model_200_RGB_sequence_patch_augmentation_cp.h5\n",
            " 20210312-144611_deepest_lstm_tinypilotnet_model_200_RGB_sequence_patch_augmentation.csv\n",
            " 20210312-144611_deepest_lstm_tinypilotnet_model_200_RGB_sequence_patch_augmentation.h5\n",
            " 20210312-153411_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch_augmentation_cp.h5\n",
            " 20210312-153411_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch_augmentation.csv\n",
            " 20210312-153411_deepest_lstm_tinypilotnet_model_300_RGB_sequence_patch_augmentation.h5\n",
            " 20210315-174029_deepest_lstm_tinypilotnet_model_50_RGB_sequence_gaussian_augmentation_cp.h5\n",
            " 20210315-174029_deepest_lstm_tinypilotnet_model_50_RGB_sequence_gaussian_augmentation.csv\n",
            " 20210315-174935_deepest_lstm_tinypilotnet_model_300_RGB_sequence_gaussian_augmentation_cp.h5\n",
            " 20210315-174935_deepest_lstm_tinypilotnet_model_300_RGB_sequence_gaussian_augmentation.csv\n",
            " 20210315-174935_deepest_lstm_tinypilotnet_model_300_RGB_sequence_gaussian_augmentation.h5\n",
            " 20210316-130244_lenet_model_300_RGB_extreme_cp.h5\n",
            " 20210316-130244_lenet_model_300_RGB_extreme.csv\n",
            " AI-exam.zip\n",
            " certificados-horas-formativas-doctorado\n",
            "'Colab Notebooks'\n",
            " complete_dataset.zip\n",
            " curves_only.zip\n",
            " DetectionStudio\n",
            "'documentacioÃÅn doctorado'\n",
            " Draw.io\n",
            "'Fotos Film'\n",
            " many_curves_dataset.zip\n",
            " merged_model_pilotnet_cropped_100_dense_1.h5\n",
            " merged_model_pilotnet_cropped_100_dense_2.h5\n",
            " merged_model_pilotnet_cropped_100.h5\n",
            " merged_model_tinypilotnet_cropped_100.h5\n",
            " merged_model_tinypilotnet_cropped_300.h5\n",
            " model_deepest_lstm_cropped_250_norm_max_pooling.h5\n",
            " model_deepest_lstm_cropped_250_norm_test.h5\n",
            " model_lstm_cropped_1.h5\n",
            " model_lstm_cropped_1_test.h5\n",
            " model_lstm_cropped_250_checkpoint_norm_test_lstm.h5\n",
            " model_lstm_cropped_250_norm_test_lstm.h5\n",
            " model_lstm_cropped_300_norm.h5\n",
            " model_lstm_cropped_500_norm_test_lstm.h5\n",
            " model_lstm_sequence_dataset.h5\n",
            " model_lstm_sequence_test_cp.h5\n",
            " model_lstm_sequence_test.csv\n",
            " model_lstm_sequence_test.h5\n",
            " model_lstm_tinypilotnet_checkpoint_1000.h5\n",
            " model_lstm_tinypilotnet_checkpoint_100.h5\n",
            " model_lstm_tinypilotnet_checkpoint_1200.h5\n",
            " model_lstm_tinypilotnet_checkpoint_1700.h5\n",
            " model_lstm_tinypilotnet_checkpoint_500.h5\n",
            " model_lstm_tinypilotnet_cropped_150.h5\n",
            " model_lstm_tinypilotnet_cropped_150_v.h5\n",
            " model_lstm_tinypilotnet_cropped_150_w.h5\n",
            " model_lstm_tinypilotnet_cropped_25.h5\n",
            " model_lstm_tinypilotnet_cropped_50.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_1000.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_100.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_10_2.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_150.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_200.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_300_2.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_50_2.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_norm_300_red.h5\n",
            " model_lstm_tinypilotnet_cropped_shuffle_norm_500.h5\n",
            " model_pilotnet_cropped_300.h5\n",
            " model_pilotnet_cropped_300_v.h5\n",
            " model_pilotnet_cropped_300_w.h5\n",
            " model_pilotnet_cropped_v.h5\n",
            " model_pilotnet_cropped_w.h5\n",
            " model_plot.png\n",
            " model_smaller_vgg_7classes_normal_w.h5\n",
            " model_smaller_vgg.png\n",
            " model_tinypilotnet_cropped_v.h5\n",
            " model_tinypilotnet_cropped_w.h5\n",
            " montmelo_data.zip\n",
            " Personal\n",
            " recovery-keys\n",
            " simple_circuit_dataset.zip\n",
            " test_model_tf_keras_balanced_cropped_v.h5\n",
            " test_model_tf_keras_balanced_croppedv.h5\n",
            " test_model_tf_keras_balanced_croppedw.h5\n",
            " test_model_tf_keras_balanced_v.h5\n",
            " test_model_tf_keras_balanced_w.h5\n",
            " test_model_tf_keras_cropped_biased_v.h5\n",
            " test_model_tf_keras_cropped_biased_w.h5\n",
            " test_model_tf_keras.h5\n",
            " test_model_tf_keras_v.h5\n",
            " test_model_tf_keras_w.h5\n",
            " URJC\n",
            " websim-2-robots.mov\n",
            "'/content/drive/My Drive/complete_dataset.zip'\n",
            "Archive:  /content/drive/My Drive/curves_only.zip\n",
            "replace curves_only/data.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  /content/drive/My Drive/complete_dataset.zip\n",
            "replace complete_dataset/Train_balanced_bbdd_v/train.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5OgFc1vnBUT",
        "outputId": "81c0f399-5b7f-4ca1-c701-2a1d25a6dbb8"
      },
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import plot_model, np_utils\n",
        "\n",
        "def load_data(folder):\n",
        "    name_folder = '/content/' + folder + '/Images/'\n",
        "    list_images = glob.glob(name_folder + '*')\n",
        "    #print(list_images)\n",
        "    images = sorted(list_images, key=lambda x: int(x.split('/')[4].split('.png')[0]))\n",
        "    name_file = '/content/' + folder + '/data.json'\n",
        "    file = open(name_file, 'r')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return images, data\n",
        "\n",
        "\n",
        "def get_images(list_images, type_image, array_imgs):\n",
        "    # Read the images\n",
        "    for name in list_images:\n",
        "        img = cv2.imread(name)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        if type_image == 'cropped':\n",
        "            img = img[240:480, 0:640]\n",
        "        img = cv2.resize(img, (int(img.shape[1] / 4), int(img.shape[0] / 4)))\n",
        "        array_imgs.append(img)\n",
        "\n",
        "    return array_imgs\n",
        "\n",
        "\n",
        "def parse_json_7_classes_w(data):\n",
        "    # Process json 7 classes for w\n",
        "    array_class = []\n",
        "    data_parse = data.split('\"class2\": \"')[1:]\n",
        "    for d in data_parse:\n",
        "        classification = d.split('\", \"')[0]\n",
        "        array_class.append(classification)\n",
        "    \n",
        "    return array_class\n",
        "\n",
        "def parse_json_5_classes_v(data):\n",
        "    # Process json 5 classes for v\n",
        "    array_class = []\n",
        "    data_parse = data.split('\"class3\": \"')[1:]\n",
        "    for d in data_parse:\n",
        "        classification = d.split('\", \"')[0]\n",
        "        array_class.append(classification)\n",
        "    return array_class\n",
        "\n",
        "def parse_json(data, array):\n",
        "    array_class_w = parse_json_7_classes_w(data)\n",
        "    array_class_v = parse_json_5_classes_v(data)\n",
        "\n",
        "    #array_class_v_w = []\n",
        "    #for i in range(0, len(array_class_w)):\n",
        "      #array_class_v_w.append([array_class_v[i], array_class_w[i]])\n",
        "\n",
        "    for i in range(0, len(array_class_w)):\n",
        "      array.append([array_class_v[i], array_class_w[i]])\n",
        "\n",
        "    # array_class = [array_class_v, array_class_w]\n",
        "    # array_class = adapt_labels(array_class_v_w, array)\n",
        "\n",
        "    return array\n",
        "\n",
        "\n",
        "def adapt_labels(array_labels, array_classes):\n",
        "  for i in range(0, len(array_labels)):\n",
        "    label_v = array_labels[i][0]\n",
        "    label_w = array_labels[i][1]\n",
        "    if label_v == 'slow' and label_w == 'radically_left':\n",
        "      array_classes.append(0)\n",
        "    elif label_v == 'slow' and label_w == 'moderately_left':\n",
        "      array_classes.append(1)\n",
        "    elif label_v == 'slow' and label_w == 'slightly_left':\n",
        "      array_classes.append(2)\n",
        "    elif label_v == 'slow' and label_w == 'slight':\n",
        "      array_classes.append(3)\n",
        "    elif label_v == 'slow' and label_w == 'slightly_right':\n",
        "      array_classes.append(4)\n",
        "    elif label_v == 'slow' and label_w == 'moderately_right':\n",
        "      array_classes.append(5)\n",
        "    elif label_v == 'slow' and label_w == 'radically_right':\n",
        "      array_classes.append(6)\n",
        "    elif label_v == 'moderate' and label_w == 'radically_left':\n",
        "      array_classes.append(7)\n",
        "    elif label_v == 'moderate' and label_w == 'moderately_left':\n",
        "      array_classes.append(8)\n",
        "    elif label_v == 'moderate' and label_w == 'slightly_left':\n",
        "      array_classes.append(9)\n",
        "    elif label_v == 'moderate' and label_w == 'slight':\n",
        "      array_classes.append(10)\n",
        "    elif label_v == 'moderate' and label_w == 'slightly_right':\n",
        "      array_classes.append(11)\n",
        "    elif label_v == 'moderate' and label_w == 'moderately_right':\n",
        "      array_classes.append(12)\n",
        "    elif label_v == 'moderate' and label_w == 'radically_right':\n",
        "      array_classes.append(13)\n",
        "    elif label_v == 'fast' and label_w == 'radically_left':\n",
        "      array_classes.append(14)\n",
        "    elif label_v == 'fast' and label_w == 'moderately_left':\n",
        "      array_classes.append(15)\n",
        "    elif label_v == 'fast' and label_w == 'slightly_left':\n",
        "      array_classes.append(16)\n",
        "    elif label_v == 'fast' and label_w == 'slight':\n",
        "      array_classes.append(17)\n",
        "    elif label_v == 'fast' and label_w == 'slightly_right':\n",
        "      array_classes.append(18)\n",
        "    elif label_v == 'fast' and label_w == 'moderately_right':\n",
        "      array_classes.append(19)\n",
        "    elif label_v == 'fast' and label_w == 'radically_right':\n",
        "      array_classes.append(20)\n",
        "    elif label_v == 'very_fast' and label_w == 'radically_left':\n",
        "      array_classes.append(21)\n",
        "    elif label_v == 'very_fast' and label_w == 'moderately_left':\n",
        "      array_classes.append(22)\n",
        "    elif label_v == 'very_fast' and label_w == 'slightly_left':\n",
        "      array_classes.append(23)\n",
        "    elif label_v == 'very_fast' and label_w == 'slight':\n",
        "      array_classes.append(24)\n",
        "    elif label_v == 'very_fast' and label_w == 'slightly_right':\n",
        "      array_classes.append(25)\n",
        "    elif label_v == 'very_fast' and label_w == 'moderately_right':\n",
        "      array_classes.append(26)\n",
        "    elif label_v == 'very_fast' and label_w == 'radically_right':\n",
        "      array_classes.append(27)\n",
        "  \n",
        "  return array_classes\n",
        "\n",
        "\n",
        "def preprocess_data(array, imgs):\n",
        "    # Data augmentation\n",
        "    # Take the image and just flip it and negate the measurement\n",
        "    flip_imgs = []\n",
        "    array_flip = []\n",
        "    for i in range(len(array)):\n",
        "        flip_imgs.append(cv2.flip(imgs[i], 1))\n",
        "        if array[i][1] == 'radically_left':\n",
        "          w = 'radically_right'\n",
        "        elif array[i][1] == 'moderately_left':\n",
        "          w = 'moderately_right'\n",
        "        elif array[i][1] == 'slightly_left':\n",
        "          w = 'slightly_right'\n",
        "        elif array[i][1] == 'slight':\n",
        "          w = 'slight'\n",
        "        elif array[i][1] == 'slightly_right':\n",
        "          w = 'slightly_left'\n",
        "        elif array[i][1] == 'moderately_right':\n",
        "          w = 'moderately_left'\n",
        "        elif array[i][1] == 'radically_right':\n",
        "          w = 'radically_left'\n",
        "        array_flip.append((array[i][0], w))\n",
        "    new_array = array + array_flip\n",
        "    new_array_imgs = imgs + flip_imgs\n",
        "    return new_array, new_array_imgs\n",
        "\n",
        "def add_extreme_data(array, imgs):\n",
        "    for i in range(0, len(array)):\n",
        "      if array[i][1] == 'radically_right' or array[i][1] == 'radically_left':\n",
        "        array.append(array[i])\n",
        "        imgs.append(imgs[i])\n",
        "    return array, imgs\n",
        "\n",
        "\n",
        "# Load data\n",
        "images, data = load_data('complete_dataset')\n",
        "images_curve, data_curve = load_data('curves_only')\n",
        "\n",
        "# CHANGE type_image\n",
        "type_image = 'cropped'\n",
        "#type_image='normal'\n",
        "\n",
        "# Preprocess images\n",
        "array_imgs = []\n",
        "array_imgs = get_images(images, type_image, array_imgs)\n",
        "array_imgs = get_images(images_curve, type_image, array_imgs)\n",
        "# Preprocess json\n",
        "array_annotations = []\n",
        "array_annotations = parse_json(data, array_annotations)\n",
        "print(len(array_annotations))\n",
        "array_annotations = parse_json(data_curve, array_annotations)\n",
        "print(len(array_annotations))\n",
        "\n",
        "\n",
        "if type_image == 'cropped':\n",
        "    img_shape = (65, 160, 3)\n",
        "else:\n",
        "    img_shape = (120, 160, 3)\n",
        "\n",
        "\n",
        "# Adapt the data\n",
        "array_annotations, array_imgs = preprocess_data(array_annotations, array_imgs)\n",
        "print(len(array_annotations))\n",
        "array_annotations, array_imgs = add_extreme_data(array_annotations, array_imgs)\n",
        "print(len(array_annotations))\n",
        "\n",
        "\n",
        "array_classes = []\n",
        "array_classes = adapt_labels(array_annotations, array_classes)\n",
        "print(len(array_classes))\n",
        "print(array_classes[0])\n",
        "\n",
        "split_test_train_value = 0.30\n",
        "images_train, images_validation, classes_train, classes_validation = train_test_split(array_imgs, array_classes, test_size=split_test_train_value, random_state=42, shuffle=True)\n",
        "# Adapt the data\n",
        "images_train = np.stack(images_train, axis=0)\n",
        "images_validation = np.stack(images_validation, axis=0)\n",
        "\n",
        "classes_train = np_utils.to_categorical(classes_train, 28)\n",
        "classes_validation = np_utils.to_categorical(classes_validation, 28)\n",
        "\n",
        "classes_train = np.stack(classes_train, axis=0)\n",
        "classes_validation = np.stack(classes_validation, axis=0)\n",
        "\n",
        "\n",
        "print(classes_train[0])\n",
        "print(classes_train.shape)\n",
        "\n",
        "\n",
        "print(images_train.shape)\n",
        "print(images_validation.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17341\n",
            "22609\n",
            "45218\n",
            "49378\n",
            "49378\n",
            "24\n",
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "(34564, 28)\n",
            "(34564, 60, 160, 3)\n",
            "(14814, 60, 160, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6At55Lt_SP7n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "outputId": "59271b91-b384-4b21-a33a-d8a536da7e33"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(classes_train[100], classes_validation[100])\n",
        "\n",
        "\n",
        "plt.show()\n",
        "n, bins, patches = plt.hist(x=array_classes, bins='auto')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#n, bins, patches = plt.hist(x=train_ann_x, bins='auto')\n",
        "plt.show()\n",
        "n, bins, patches = plt.hist(x=classes_train, bins='auto')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "plt.show()\n",
        "n, bins, patches = plt.hist(x=classes_validation, bins='auto')\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWicffveSUxd"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def lenet(input_shape):\n",
        "\n",
        "    model = Sequential()\n",
        "    #model.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(32,32,1)))\n",
        "    model.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(AveragePooling2D())\n",
        "    model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(AveragePooling2D())\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(units=120, activation='relu'))\n",
        "    model.add(Dense(units=84, activation='relu'))\n",
        "    model.add(Dense(units=28, activation = 'sigmoid'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    '''\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(units=28, activation='sigmoid'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    '''\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buioIMZtSd6P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f362b08-2613-4b2c-8f3f-d83a2a447d41"
      },
      "source": [
        "import time\n",
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "print(timestr)\n",
        "print(images_train[0].shape)\n",
        "# print(annotations_train[0])\n",
        "#print(img_shape)\n",
        "img_shape = (60, 160, 3)\n",
        "print(img_shape)\n",
        "\n",
        "\n",
        "timesteps = 1\n",
        "batch_size = 50  # 16\n",
        "nb_epoch = 100  # 223\n",
        "model_name = 'lenet'\n",
        "model = lenet(img_shape)\n",
        "model_filename = timestr + '_lenet_model_100_RGB_extreme'\n",
        "\n",
        "\n",
        "if type_image == 'cropped':\n",
        "    model_file = '/content/drive/My Drive/' + model_filename + '.h5'\n",
        "\n",
        "\n",
        "# Print layers\n",
        "print(model)\n",
        "model.build(img_shape)\n",
        "print(model.summary())\n",
        "\n",
        "import datetime\n",
        "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, CSVLogger\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "earlystopping=EarlyStopping(monitor=\"accuracy\", patience=40, verbose=1, mode='auto')\n",
        "# Create a callback that saves the model's weights\n",
        "checkpoint_path = \"/content/drive/My Drive/\" + model_filename + '_cp.h5'\n",
        "cp_callback = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, verbose=1)\n",
        "csv_logger = CSVLogger(\"/content/drive/My Drive/\" + model_filename + '.csv', append=True)\n",
        "\n",
        "\n",
        "# Train\n",
        "model_history = model.fit(images_train, classes_train, epochs=nb_epoch, batch_size=batch_size, verbose=2, validation_data=(images_validation, classes_validation), callbacks=[tensorboard_callback, earlystopping, cp_callback, csv_logger])\n",
        "\n",
        "# Save the model\n",
        "model.save(model_file)\n",
        "\n",
        "\n",
        "# Evaluate the modelpil\n",
        "score = model.evaluate(images_validation, classes_validation, verbose=0)\n",
        "\n",
        "print('Evaluating')\n",
        "print('Test loss: ', score[0])\n",
        "print('Test mean squared error: ', score[1])\n",
        "print('Test mean absolute error: ', score[2])\n",
        "\n",
        "\n",
        "# SAVE METADATA\n",
        "from tensorflow.python.keras.saving import hdf5_format\n",
        "import h5py\n",
        "\n",
        "model_path = model_file\n",
        "# Save model\n",
        "with h5py.File(model_path, mode='w') as f:\n",
        "    hdf5_format.save_model_to_hdf5(model, f)\n",
        "    f.attrs['experiment_name'] = ''\n",
        "    f.attrs['experiment_description'] = ''\n",
        "    f.attrs['batch_size'] = batch_size\n",
        "    f.attrs['nb_epoch'] = nb_epoch\n",
        "    f.attrs['model'] = model_name\n",
        "    f.attrs['img_shape'] = img_shape\n",
        "    f.attrs['normalized_dataset'] = True\n",
        "    f.attrs['sequences_dataset'] = True\n",
        "    f.attrs['gpu_trained'] = True\n",
        "    f.attrs['data_augmentation'] = True\n",
        "    f.attrs['extreme_data'] = False\n",
        "    f.attrs['split_test_train'] = 0.30\n",
        "    f.attrs['instances_number'] = len(array_annotations)\n",
        "    f.attrs['loss'] = score[0]\n",
        "    f.attrs['mse'] = score[1]\n",
        "    f.attrs['mae'] = score[2]\n",
        "    f.attrs['csv_path'] = model_filename + '.csv'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20210316-140343\n",
            "(60, 160, 3)\n",
            "(60, 160, 3)\n",
            "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f3c6d6352d0>\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 58, 158, 6)        168       \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 29, 79, 6)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 27, 77, 16)        880       \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 13, 38, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 7904)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 120)               948600    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 84)                10164     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 28)                2380      \n",
            "=================================================================\n",
            "Total params: 962,192\n",
            "Trainable params: 962,192\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "692/692 - 8s - loss: 1.2636 - accuracy: 0.7260 - val_loss: 0.5640 - val_accuracy: 0.8137\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.56395, saving model to /content/drive/My Drive/20210316-140343_lenet_model_100_RGB_extreme_cp.h5\n",
            "Epoch 2/100\n",
            "692/692 - 6s - loss: 0.4437 - accuracy: 0.8467 - val_loss: 0.4196 - val_accuracy: 0.8504\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.56395 to 0.41957, saving model to /content/drive/My Drive/20210316-140343_lenet_model_100_RGB_extreme_cp.h5\n",
            "Epoch 3/100\n",
            "692/692 - 6s - loss: 0.3507 - accuracy: 0.8734 - val_loss: 0.3806 - val_accuracy: 0.8655\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.41957 to 0.38059, saving model to /content/drive/My Drive/20210316-140343_lenet_model_100_RGB_extreme_cp.h5\n",
            "Epoch 4/100\n",
            "692/692 - 6s - loss: 0.3068 - accuracy: 0.8915 - val_loss: 0.3610 - val_accuracy: 0.8771\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.38059 to 0.36104, saving model to /content/drive/My Drive/20210316-140343_lenet_model_100_RGB_extreme_cp.h5\n",
            "Epoch 5/100\n",
            "692/692 - 6s - loss: 0.2629 - accuracy: 0.9055 - val_loss: 0.3511 - val_accuracy: 0.8798\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.36104 to 0.35113, saving model to /content/drive/My Drive/20210316-140343_lenet_model_100_RGB_extreme_cp.h5\n",
            "Epoch 6/100\n",
            "692/692 - 6s - loss: 0.2374 - accuracy: 0.9135 - val_loss: 0.3509 - val_accuracy: 0.8881\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.35113 to 0.35092, saving model to /content/drive/My Drive/20210316-140343_lenet_model_100_RGB_extreme_cp.h5\n",
            "Epoch 7/100\n",
            "692/692 - 6s - loss: 0.2127 - accuracy: 0.9244 - val_loss: 0.3486 - val_accuracy: 0.8882\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.35092 to 0.34859, saving model to /content/drive/My Drive/20210316-140343_lenet_model_100_RGB_extreme_cp.h5\n",
            "Epoch 8/100\n",
            "692/692 - 6s - loss: 0.1972 - accuracy: 0.9294 - val_loss: 0.3412 - val_accuracy: 0.8928\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.34859 to 0.34120, saving model to /content/drive/My Drive/20210316-140343_lenet_model_100_RGB_extreme_cp.h5\n",
            "Epoch 9/100\n",
            "692/692 - 6s - loss: 0.1720 - accuracy: 0.9383 - val_loss: 0.3876 - val_accuracy: 0.8837\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.34120\n",
            "Epoch 10/100\n",
            "692/692 - 6s - loss: 0.1659 - accuracy: 0.9410 - val_loss: 0.3476 - val_accuracy: 0.9023\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.34120\n",
            "Epoch 11/100\n",
            "692/692 - 6s - loss: 0.1456 - accuracy: 0.9488 - val_loss: 0.4021 - val_accuracy: 0.8905\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.34120\n",
            "Epoch 12/100\n",
            "692/692 - 6s - loss: 0.1340 - accuracy: 0.9528 - val_loss: 0.4139 - val_accuracy: 0.8962\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.34120\n",
            "Epoch 13/100\n",
            "692/692 - 6s - loss: 0.1232 - accuracy: 0.9576 - val_loss: 0.4042 - val_accuracy: 0.8994\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.34120\n",
            "Epoch 14/100\n",
            "692/692 - 6s - loss: 0.1189 - accuracy: 0.9588 - val_loss: 0.3893 - val_accuracy: 0.9041\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.34120\n",
            "Epoch 15/100\n",
            "692/692 - 6s - loss: 0.1063 - accuracy: 0.9624 - val_loss: 0.4013 - val_accuracy: 0.9023\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.34120\n",
            "Epoch 16/100\n",
            "692/692 - 6s - loss: 0.0914 - accuracy: 0.9687 - val_loss: 0.4786 - val_accuracy: 0.8954\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.34120\n",
            "Epoch 17/100\n",
            "692/692 - 6s - loss: 0.0915 - accuracy: 0.9682 - val_loss: 0.4578 - val_accuracy: 0.9031\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.34120\n",
            "Epoch 18/100\n",
            "692/692 - 6s - loss: 0.0771 - accuracy: 0.9739 - val_loss: 0.4797 - val_accuracy: 0.8985\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.34120\n",
            "Epoch 19/100\n",
            "692/692 - 6s - loss: 0.0780 - accuracy: 0.9724 - val_loss: 0.5199 - val_accuracy: 0.8989\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.34120\n",
            "Epoch 20/100\n",
            "692/692 - 6s - loss: 0.0751 - accuracy: 0.9743 - val_loss: 0.5096 - val_accuracy: 0.9012\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.34120\n",
            "Epoch 21/100\n",
            "692/692 - 6s - loss: 0.0705 - accuracy: 0.9764 - val_loss: 0.5315 - val_accuracy: 0.9002\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.34120\n",
            "Epoch 22/100\n",
            "692/692 - 6s - loss: 0.0623 - accuracy: 0.9788 - val_loss: 0.5599 - val_accuracy: 0.9033\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.34120\n",
            "Epoch 23/100\n",
            "692/692 - 6s - loss: 0.0554 - accuracy: 0.9814 - val_loss: 0.6378 - val_accuracy: 0.8995\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.34120\n",
            "Epoch 24/100\n",
            "692/692 - 6s - loss: 0.0608 - accuracy: 0.9802 - val_loss: 0.6253 - val_accuracy: 0.9013\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.34120\n",
            "Epoch 25/100\n",
            "692/692 - 6s - loss: 0.0573 - accuracy: 0.9819 - val_loss: 0.6367 - val_accuracy: 0.8991\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.34120\n",
            "Epoch 26/100\n",
            "692/692 - 6s - loss: 0.0497 - accuracy: 0.9834 - val_loss: 0.6252 - val_accuracy: 0.9043\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.34120\n",
            "Epoch 27/100\n",
            "692/692 - 6s - loss: 0.0507 - accuracy: 0.9836 - val_loss: 0.6228 - val_accuracy: 0.9035\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.34120\n",
            "Epoch 28/100\n",
            "692/692 - 6s - loss: 0.0383 - accuracy: 0.9885 - val_loss: 0.6140 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.34120\n",
            "Epoch 29/100\n",
            "692/692 - 6s - loss: 0.0525 - accuracy: 0.9836 - val_loss: 0.6155 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.34120\n",
            "Epoch 30/100\n",
            "692/692 - 6s - loss: 0.0504 - accuracy: 0.9839 - val_loss: 0.6481 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.34120\n",
            "Epoch 31/100\n",
            "692/692 - 6s - loss: 0.0443 - accuracy: 0.9866 - val_loss: 0.6650 - val_accuracy: 0.8985\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.34120\n",
            "Epoch 32/100\n",
            "692/692 - 6s - loss: 0.0315 - accuracy: 0.9905 - val_loss: 0.6341 - val_accuracy: 0.9016\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.34120\n",
            "Epoch 33/100\n",
            "692/692 - 7s - loss: 0.0433 - accuracy: 0.9869 - val_loss: 0.6983 - val_accuracy: 0.8996\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.34120\n",
            "Epoch 34/100\n",
            "692/692 - 7s - loss: 0.0376 - accuracy: 0.9886 - val_loss: 0.8159 - val_accuracy: 0.8946\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.34120\n",
            "Epoch 35/100\n",
            "692/692 - 6s - loss: 0.0412 - accuracy: 0.9883 - val_loss: 0.7870 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.34120\n",
            "Epoch 36/100\n",
            "692/692 - 6s - loss: 0.0434 - accuracy: 0.9874 - val_loss: 0.7173 - val_accuracy: 0.8989\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.34120\n",
            "Epoch 37/100\n",
            "692/692 - 6s - loss: 0.0241 - accuracy: 0.9928 - val_loss: 0.7061 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.34120\n",
            "Epoch 38/100\n",
            "692/692 - 6s - loss: 0.0295 - accuracy: 0.9906 - val_loss: 0.7409 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.34120\n",
            "Epoch 39/100\n",
            "692/692 - 6s - loss: 0.0317 - accuracy: 0.9905 - val_loss: 0.8115 - val_accuracy: 0.9009\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.34120\n",
            "Epoch 40/100\n",
            "692/692 - 6s - loss: 0.0227 - accuracy: 0.9926 - val_loss: 0.7969 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.34120\n",
            "Epoch 41/100\n",
            "692/692 - 7s - loss: 0.0488 - accuracy: 0.9853 - val_loss: 0.7477 - val_accuracy: 0.9000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.34120\n",
            "Epoch 42/100\n",
            "692/692 - 7s - loss: 0.0327 - accuracy: 0.9903 - val_loss: 0.7542 - val_accuracy: 0.9012\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.34120\n",
            "Epoch 43/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-aef2122953ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mmodel_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearlystopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcp_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKQjwTMpKL-C"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}